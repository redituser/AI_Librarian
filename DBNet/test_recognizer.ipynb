{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98745821-20df-4ee3-926e-25637d1faf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 텍스트 인식(Recognition) 테스트 시작 =====\n",
      "사용 장치: cuda\n",
      "\n",
      ">> 학습 데이터(gt.txt)에서 문자셋을 생성합니다...\n",
      "\n",
      ">> 'crnn_recognizer_final.pth' 모델을 로드합니다...\n",
      "모델 로드 완료.\n",
      "\n",
      ">> 'test03.jpg' 이미지의 텍스트를 예측합니다...\n",
      "\n",
      "==================== 최종 결과 ====================\n",
      "입력 이미지: test03.jpg\n",
      "모델 예측 텍스트: '실'\n",
      "=====================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_21940\\2571165164.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "\n",
    "\n",
    "# 1) 학습 완료된 모델 파일의 경로\n",
    "MODEL_PATH = \"crnn_recognizer_final.pth\"\n",
    "\n",
    "# 2) 테스트하고 싶은 이미지 한 장의 경로\n",
    "#    (인식 모델이므로, 글자 부분만 잘려진 이미지를 넣어야 합니다.)\n",
    "IMAGE_PATH = \"test03.jpg\" # <-- 테스트할 이미지 경로로 수정!\n",
    "\n",
    "# 3) 학습에 사용했던 gt.txt 파일의 경로 (CHARACTER_SET 자동 생성을 위해 필요)\n",
    "GT_FILE_FOR_CHARSET = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\gt.txt\" \n",
    "\n",
    "# 4) 실행 장치 설정\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_character_set(gt_file):\n",
    "    \"\"\"gt.txt 파일로부터 CHARACTER_SET을 자동으로 생성합니다.\"\"\"\n",
    "    if not os.path.exists(gt_file):\n",
    "        print(f\"오류: gt.txt 파일을 찾을 수 없습니다! 경로를 확인하세요: {gt_file}\")\n",
    "        return None\n",
    "    all_characters = set()\n",
    "    with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                _, text = line.strip().split('\\t', 1)\n",
    "                for char in text:\n",
    "                    all_characters.add(char)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    sorted_characters = sorted(list(all_characters))\n",
    "    final_charset = \"\".join(sorted_characters)\n",
    "    return final_charset\n",
    "\n",
    "class CTCLabelConverter:\n",
    "    \"\"\"텍스트와 숫자 인덱스 간 변환기\"\"\"\n",
    "    def __init__(self, character_set):\n",
    "        self.character_set = [\"-\"] + list(character_set)\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.character_set)}\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.character_set)}\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        text = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx_item = idx.item()\n",
    "            if idx_item == 0: last_idx = 0; continue\n",
    "            if idx_item == last_idx: continue\n",
    "            text.append(self.idx_to_char[idx_item])\n",
    "            last_idx = idx_item\n",
    "        return \"\".join(text)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return len(self.character_set)\n",
    "\n",
    "def get_recognition_transforms(height, width):\n",
    "    \"\"\"테스트용 이미지 변환기\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height, width),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"CRNN 모델 아키텍처 (학습 때와 동일)\"\"\"\n",
    "    def __init__(self, num_chars, rnn_hidden_size=256, rnn_layers=2):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(512, 512, (2,1), 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=512, hidden_size=rnn_hidden_size, num_layers=rnn_layers, bidirectional=True, dropout=0.5)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_chars)\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x); b, c, h, w = features.size()\n",
    "        assert h == 1, \"CNN 출력의 높이는 1이어야 합니다.\"\n",
    "        features = features.squeeze(2).permute(2, 0, 1); rnn_output, _ = self.rnn(features)\n",
    "        return self.classifier(rnn_output)\n",
    "\n",
    "class Recognizer:\n",
    "    \"\"\"학습된 모델로 추론을 수행하는 클래스\"\"\"\n",
    "    def __init__(self, model_path, converter, device, img_height=32, img_width=100):\n",
    "        self.device = device; self.converter = converter\n",
    "        num_classes = self.converter.get_num_classes()\n",
    "        self.model = CRNN(num_chars=num_classes).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        self.transform = get_recognition_transforms(img_height, img_width)\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None: return f\"이미지 파일을 찾거나 열 수 없습니다: {image_path}\"\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            return f\"이미지 로드 오류: {e}\"\n",
    "        image_tensor = self.transform(image=image)['image'].unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(image_tensor)\n",
    "            preds_idx = preds.argmax(2).permute(1, 0)\n",
    "        decoded_text = self.converter.decode(preds_idx[0])\n",
    "        return decoded_text\n",
    "\n",
    "# ====================================================================================\n",
    "# 메인 실행 블록\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"===== 텍스트 인식(Recognition) 테스트 시작 =====\")\n",
    "    print(f\"사용 장치: {DEVICE}\")\n",
    "\n",
    "    # 1. 학습에 사용된 글자셋(CHARACTER_SET) 자동 생성\n",
    "    print(f\"\\n>> 학습 데이터({os.path.basename(GT_FILE_FOR_CHARSET)})에서 문자셋을 생성합니다...\")\n",
    "    charset = generate_character_set(GT_FILE_FOR_CHARSET)\n",
    "    \n",
    "    if charset:\n",
    "        # 2. 추론기(Recognizer) 초기화\n",
    "        try:\n",
    "            print(f\"\\n>> '{MODEL_PATH}' 모델을 로드합니다...\")\n",
    "            recognizer = Recognizer(MODEL_PATH, CTCLabelConverter(charset), DEVICE)\n",
    "            print(\"모델 로드 완료.\")\n",
    "            \n",
    "            # 3. 이미지 예측 실행\n",
    "            print(f\"\\n>> '{os.path.basename(IMAGE_PATH)}' 이미지의 텍스트를 예측합니다...\")\n",
    "            predicted_text = recognizer.predict(IMAGE_PATH)\n",
    "            \n",
    "            # 4. 최종 결과 출력\n",
    "            print(\"\\n\" + \"=\"*20 + \" 최종 결과 \" + \"=\"*20)\n",
    "            print(f\"입력 이미지: {IMAGE_PATH}\")\n",
    "            print(f\"모델 예측 텍스트: '{predicted_text}'\")\n",
    "            print(\"=\"*53)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\n[오류] 모델 파일 또는 이미지 파일을 찾을 수 없습니다. 상단의 경로 설정을 확인해주세요.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n예상치 못한 오류가 발생했습니다: {e}\")\n",
    "    else:\n",
    "        print(\"\\n[오류] 문자셋 생성에 실패하여 테스트를 진행할 수 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.10 (Py3.10)",
   "language": "python",
   "name": "tensortf210py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
