{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d903f-536e-493d-b4e0-73b0ddad8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# 0. 라이브러리 임포트  학습용 코드\n",
    "# ====================================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 1: 텍스트 탐지 모듈 (DBNet) 및 손실 함수 (DBLoss)\n",
    "# ====================================================================================\n",
    "\n",
    "# 1-1. DBNet 아키텍처\n",
    "class MobileNetV3_Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        features = mobilenet_v3_large(pretrained=pretrained).features\n",
    "        self.out_layers = nn.ModuleList([\n",
    "            features[0:4], features[4:7], features[7:13], features[13:17]\n",
    "        ])\n",
    "        self.out_channels = [40, 80, 160, 960]\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [layer(x) for layer in self.out_layers]\n",
    "        return outputs\n",
    "\n",
    "class DB_FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.in_convs = nn.ModuleList([nn.Conv2d(c, out_channels, 1, bias=False) for c in in_channels])\n",
    "        self.out_convs = nn.ModuleList([nn.Conv2d(out_channels, out_channels // 4, 3, padding=1, bias=False) for _ in in_channels])\n",
    "\n",
    "    def forward(self, features):\n",
    "        inner_features = [conv(f) for conv, f in zip(self.in_convs, features)]\n",
    "        p4 = inner_features[3]\n",
    "        p3 = F.interpolate(p4, scale_factor=2) + inner_features[2]\n",
    "        p2 = F.interpolate(p3, scale_factor=2) + inner_features[1]\n",
    "        p1 = F.interpolate(p2, scale_factor=2) + inner_features[0]\n",
    "        \n",
    "        final_features = [\n",
    "            self.out_convs[0](p1),\n",
    "            self.out_convs[1](F.interpolate(p2, scale_factor=2)),\n",
    "            self.out_convs[2](F.interpolate(p3, scale_factor=4)),\n",
    "            self.out_convs[3](F.interpolate(p4, scale_factor=8)),\n",
    "        ]\n",
    "        return torch.cat(final_features, dim=1)\n",
    "\n",
    "class DB_Head(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super().__init__()\n",
    "        self.prob_conv = nn.Sequential(nn.Conv2d(in_channels, in_channels // 4, 3, padding=1), nn.ReLU(), nn.ConvTranspose2d(in_channels // 4, 1, 2, 2))\n",
    "        self.thresh_conv = nn.Sequential(nn.Conv2d(in_channels, in_channels // 4, 3, padding=1), nn.ReLU(), nn.ConvTranspose2d(in_channels // 4, 1, 2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        prob_map = torch.sigmoid(self.prob_conv(x))\n",
    "        thresh_map = torch.sigmoid(self.thresh_conv(x))\n",
    "        return prob_map, thresh_map\n",
    "\n",
    "class DBNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = MobileNetV3_Backbone()\n",
    "        self.fpn = DB_FPN(in_channels=self.backbone.out_channels)\n",
    "        self.head = DB_Head(in_channels=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.fpn(self.backbone(x)))\n",
    "\n",
    "# 1-2. DBNet 손실 함수\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred, target = pred * mask, target * mask\n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum() + self.eps\n",
    "        return 1 - (2 * intersection / union)\n",
    "\n",
    "class DBLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=10.0, k=50):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.k = alpha, beta, k\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, pred_maps, gt_maps):\n",
    "        pred_prob, pred_thresh = pred_maps\n",
    "        gt_prob, gt_prob_mask, gt_thresh, gt_thresh_mask = gt_maps\n",
    "\n",
    "        # L_s (BCE + Dice)\n",
    "        bce_prob_loss = (self.bce_loss(pred_prob, gt_prob) * gt_prob_mask).mean()\n",
    "        dice_prob_loss = self.dice_loss(pred_prob, gt_prob, gt_prob_mask)\n",
    "        loss_s = bce_prob_loss + dice_prob_loss\n",
    "\n",
    "        # L_b (Dice on DB map)\n",
    "        db_map = 1 / (1 + torch.exp(-self.k * (pred_prob - pred_thresh)))\n",
    "        loss_b = self.dice_loss(db_map, gt_prob, gt_prob_mask)\n",
    "\n",
    "        # L_t (L1 on Threshold map)\n",
    "        loss_t = (torch.abs(pred_thresh - gt_thresh) * gt_thresh_mask).sum() / (gt_thresh_mask.sum() + 1e-6)\n",
    "\n",
    "        return loss_s + self.alpha * loss_b + self.beta * loss_t\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 2: 텍스트 인식 모듈 (CRNN)\n",
    "# ====================================================================================\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars, rnn_hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(512, rnn_hidden_size, bidirectional=True, num_layers=2)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_features = self.cnn(x).squeeze(2).permute(2, 0, 1)\n",
    "        rnn_output, _ = self.rnn(conv_features)\n",
    "        return self.classifier(rnn_output)\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 3: 데이터 시뮬레이션 (더미 데이터 생성)\n",
    "# ====================================================================================\n",
    "def generate_dummy_ocr_image(path=\"dummy_ocr_test.png\"):\n",
    "    \"\"\"추론 테스트를 위한 더미 이미지 생성\"\"\"\n",
    "    image = np.ones((600, 800, 3), np.uint8) * 255\n",
    "    cv2.putText(image, 'hello ocr 123', (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 5)\n",
    "    cv2.putText(image, 'pytorch', (150, 400), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 5)\n",
    "    cv2.imwrite(path, image)\n",
    "    return path\n",
    "\n",
    "def generate_dummy_training_batch(batch_size, img_size, character_list):\n",
    "    \"\"\"학습 과정을 시뮬레이션하기 위한 더미 배치 데이터 생성\"\"\"\n",
    "    # 1. 탐지 모델용 데이터\n",
    "    det_images = torch.rand(batch_size, 3, img_size, img_size)\n",
    "    gt_prob = torch.rand(batch_size, 1, img_size, img_size) > 0.8\n",
    "    gt_prob_mask = torch.ones(batch_size, 1, img_size, img_size)\n",
    "    gt_thresh = torch.rand(batch_size, 1, img_size, img_size)\n",
    "    gt_thresh_mask = gt_prob.clone()\n",
    "    db_gts = (gt_prob.float(), gt_prob_mask.float(), gt_thresh.float(), gt_thresh_mask.float())\n",
    "    \n",
    "    # 2. 인식 모델용 데이터 (잘라낸 단어 이미지라고 가정)\n",
    "    rec_images = torch.rand(batch_size, 3, 32, 100) # (B, C, H, W)\n",
    "    \n",
    "    # 더미 텍스트 생성\n",
    "    gt_texts = []\n",
    "    for _ in range(batch_size):\n",
    "        length = np.random.randint(3, 10)\n",
    "        text = \"\".join(np.random.choice(list(character_list), length))\n",
    "        gt_texts.append(text)\n",
    "        \n",
    "    return det_images, db_gts, rec_images, gt_texts\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 4: OCR 파이프라인 (추론용)\n",
    "# ====================================================================================\n",
    "class OCR_Pipeline:\n",
    "    def __init__(self, detector, recognizer, char_map):\n",
    "        self.detector = detector\n",
    "        self.recognizer = recognizer\n",
    "        self.idx_to_char = {i + 1: char for i, char in enumerate(char_map)}\n",
    "        \n",
    "        self.detector.eval()\n",
    "        self.recognizer.eval()\n",
    "\n",
    "    def _get_cropped_image(self, image, box):\n",
    "        w = max(np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[2] - box[3]))\n",
    "        h = max(np.linalg.norm(box[0] - box[3]), np.linalg.norm(box[1] - box[2]))\n",
    "        dst_pts = np.array([[0, 0], [w - 1, 0], [w - 1, h - 1], [0, h - 1]], dtype=np.float32)\n",
    "        M = cv2.getPerspectiveTransform(box.astype(np.float32), dst_pts)\n",
    "        return cv2.warpPerspective(image, M, (int(w), int(h)))\n",
    "\n",
    "    def _ctc_greedy_decode(self, preds):\n",
    "        preds_idx = preds.argmax(dim=2).transpose(1, 0).contiguous().view(-1)\n",
    "        decoded_text, last_char_idx = [], 0\n",
    "        for idx in preds_idx:\n",
    "            if idx.item() == 0 or idx.item() == last_char_idx:\n",
    "                last_char_idx = idx.item() if idx.item() != 0 else 0\n",
    "                continue\n",
    "            decoded_text.append(self.idx_to_char.get(idx.item(), ''))\n",
    "            last_char_idx = idx.item()\n",
    "        return \"\".join(decoded_text)\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        img_h, img_w = image.shape[:2]\n",
    "        \n",
    "        det_input = cv2.resize(image, (640, 640))\n",
    "        det_input = torch.from_numpy(det_input).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob_map, _ = self.detector(det_input)\n",
    "            \n",
    "            # 후처리\n",
    "            binary_map = (prob_map.squeeze().numpy() > 0.3).astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            scale_x, scale_y = img_w / 640, img_h / 640\n",
    "            results = []\n",
    "            for contour in contours:\n",
    "                if cv2.contourArea(contour) < 100: continue\n",
    "                box = np.intp(cv2.boxPoints(cv2.minAreaRect(contour)) * np.array([scale_x, scale_y]))\n",
    "                \n",
    "                cropped_img = self._get_cropped_image(image, box)\n",
    "                if cropped_img.shape[0] < 10 or cropped_img.shape[1] < 10: continue\n",
    "\n",
    "                rec_input = cv2.resize(cropped_img, (100, 32))\n",
    "                rec_input = torch.from_numpy(rec_input).permute(2, 0, 1).float().unsqueeze(0) / 255.0\n",
    "                \n",
    "                preds = self.recognizer(rec_input)\n",
    "                text = self._ctc_greedy_decode(preds)\n",
    "                results.append({'box': box.tolist(), 'text': text})\n",
    "        \n",
    "        return results, image\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 5: 메인 실행 블록 (학습 시뮬레이션 및 추론)\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. 모델 및 하이퍼파라미터 설정 ---\n",
    "    print(\"Step 1: 모델 및 설정 초기화...\")\n",
    "    CHARACTER_SET = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "    NUM_CLASSES = len(CHARACTER_SET) + 1 # +1 for CTC blank token\n",
    "    \n",
    "    detector = DBNet()\n",
    "    recognizer = CRNN(num_chars=NUM_CLASSES)\n",
    "    \n",
    "    db_loss_fn = DBLoss()\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "    optimizer_det = torch.optim.Adam(detector.parameters(), lr=1e-4)\n",
    "    optimizer_rec = torch.optim.Adam(recognizer.parameters(), lr=1e-4)\n",
    "    \n",
    "    # --- 2. 더미 학습 과정 시뮬레이션 ---\n",
    "    print(\"\\nStep 2: 더미 학습 과정 시뮬레이션 시작...\")\n",
    "    num_steps = 5\n",
    "    for i in range(num_steps):\n",
    "        # 더미 데이터 생성\n",
    "        det_imgs, db_gts, rec_imgs, rec_gts = generate_dummy_training_batch(4, 640, CHARACTER_SET)\n",
    "        \n",
    "        # --- 탐지 모델 학습 ---\n",
    "        detector.train()\n",
    "        optimizer_det.zero_grad()\n",
    "        pred_maps = detector(det_imgs)\n",
    "        loss_det = db_loss_fn(pred_maps, db_gts)\n",
    "        loss_det.backward()\n",
    "        optimizer_det.step()\n",
    "        \n",
    "        # --- 인식 모델 학습 ---\n",
    "        # 실제로는 탐지된 영역을 잘라 학습하지만, 여기서는 독립적으로 시뮬레이션\n",
    "        recognizer.train()\n",
    "        optimizer_rec.zero_grad()\n",
    "        preds_rec = recognizer(rec_imgs) # (SeqLen, Batch, NumClasses)\n",
    "        \n",
    "        # CTCLoss를 위한 데이터 준비\n",
    "        log_probs = F.log_softmax(preds_rec, dim=2)\n",
    "        input_lengths = torch.full(size=(4,), fill_value=preds_rec.size(0), dtype=torch.long)\n",
    "        \n",
    "        char_to_idx = {char: i + 1 for i, char in enumerate(CHARACTER_SET)}\n",
    "        targets = torch.cat([torch.tensor([char_to_idx[c] for c in text], dtype=torch.long) for text in rec_gts])\n",
    "        target_lengths = torch.tensor([len(text) for text in rec_gts], dtype=torch.long)\n",
    "        \n",
    "        loss_rec = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "        loss_rec.backward()\n",
    "        optimizer_rec.step()\n",
    "        \n",
    "        print(f\"  Step {i+1}/{num_steps} - Detector Loss: {loss_det.item():.4f}, Recognizer Loss: {loss_rec.item():.4f}\")\n",
    "\n",
    "    print(\"더미 학습 완료.\")\n",
    "    \n",
    "    # --- 3. 추론 파이프라인 실행 ---\n",
    "    print(\"\\nStep 3: 추론 파이프라인 실행...\")\n",
    "    # '학습된' 모델로 파이프라인 교체\n",
    "    pipeline = OCR_Pipeline(detector, recognizer, CHARACTER_SET)\n",
    "    \n",
    "    # 테스트 이미지 생성 및 예측\n",
    "    dummy_img_path = generate_dummy_ocr_image()\n",
    "    ocr_results, result_image = pipeline.predict(dummy_img_path)\n",
    "    \n",
    "    print(\"\\n--- 최종 OCR 결과 ---\")\n",
    "    if not ocr_results:\n",
    "        print(\"탐지된 텍스트가 없습니다.\")\n",
    "    else:\n",
    "        for result in ocr_results:\n",
    "            # 결과 시각화\n",
    "            box = np.array(result['box'], dtype=np.intp)\n",
    "            cv2.drawContours(result_image, [box], 0, (0, 255, 0), 2)\n",
    "            # 텍스트 쓰기\n",
    "            cv2.putText(result_image, result['text'], (box[0][0], box[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            print(f\"Box: {result['box']}, Text: '{result['text']}'\")\n",
    "            \n",
    "    # 결과 이미지 저장\n",
    "    result_img_path = \"final_ocr_result.png\"\n",
    "    cv2.imwrite(result_img_path, result_image)\n",
    "    print(f\"\\n결과 이미지가 '{result_img_path}'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6308b-405f-464d-aa97-5efdfa47cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45835e4c-8f4b-4a57-b654-c28fa2f7d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3a4b8-4380-4673-b2e9-2112e902418e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cuda\n",
      "사전 학습된 MobileNetV3 가중치를 사용합니다.\n",
      "데이터 로더를 준비합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5152\\803752142.py:137: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(size, size, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 옵티마이저를 준비합니다...\n",
      "학습을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████████████████████| 2801/2801 [12:03<00:00,  3.87it/s, loss=2.0050, lr=9.9e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 완료, 평균 손실: 2.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████████████████████| 2801/2801 [12:16<00:00,  3.81it/s, loss=2.0006, lr=9.8e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 완료, 평균 손실: 2.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████████████████████| 2801/2801 [12:38<00:00,  3.69it/s, loss=2.0001, lr=9.5e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 완료, 평균 손실: 2.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|█████████████████████████████████████████| 2801/2801 [12:08<00:00,  3.85it/s, loss=2.0000, lr=9.0e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 완료, 평균 손실: 2.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████████████████████| 2801/2801 [12:30<00:00,  3.73it/s, loss=2.0000, lr=8.5e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 완료, 평균 손실: 2.0000\n",
      "Epoch 5 모델 저장 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████████████████████| 2801/2801 [12:36<00:00,  3.70it/s, loss=2.0000, lr=7.9e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 완료, 평균 손실: 2.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:   2%|▊                                          | 55/2801 [00:14<11:59,  3.81it/s, loss=2.0000, lr=7.9e-05]"
     ]
    }
   ],
   "source": [
    "#detection\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 데이터 증강 및 기하학 라이브러리\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pyclipper\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 1: 데이터 파이프라인 (Dataset, Augmentation, Ground Truth 생성)\n",
    "# ====================================================================================\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 1: 데이터 파이프라인 (최종 수정된 OcrDataset 클래스)\n",
    "# ====================================================================================\n",
    "\n",
    "class OcrDataset(Dataset):\n",
    "    def __init__(self, annotation_path, image_dir, transform=None, target_size=640):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        with open(annotation_path, 'r', encoding='utf-8') as f:\n",
    "            self.annotations = json.load(f)['images']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_info = self.annotations[idx]\n",
    "        image_key = annotation_info['file_name'] \n",
    "        image_path = os.path.join(self.image_dir, image_key)\n",
    "        \n",
    "        try:\n",
    "            # ========================[ 최종 수정 부분 ]========================\n",
    "            # cv2.imread()가 한글 경로를 처리 못하는 문제를 해결하기 위해\n",
    "            # np.fromfile과 cv2.imdecode를 사용하는 방식으로 교체합니다.\n",
    "\n",
    "            img_array = np.fromfile(image_path, np.uint8)\n",
    "            image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            if image is None:\n",
    "                # 파일을 읽었으나 디코딩에 실패한 경우\n",
    "                raise IOError(f\"Failed to decode image at {image_path}\")\n",
    "            \n",
    "            # OpenCV는 BGR로 이미지를 로드하므로, RGB로 변환합니다.\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            # =================================================================\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # 에러 발생 시, 다른 샘플을 로드하도록 시도합니다.\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        polygons_data = annotation_info.get('polygons', [])\n",
    "        \n",
    "        polygons_for_aug = []\n",
    "        for p_data in polygons_data:\n",
    "            points = p_data.get('points')\n",
    "            if points:\n",
    "                polygons_for_aug.extend(points)\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                transformed = self.transform(image=image, keypoints=polygons_for_aug)\n",
    "                image = transformed['image']\n",
    "                \n",
    "                transformed_polygons = []\n",
    "                points_per_polygon = [len(p_data.get('points', [])) for p_data in polygons_data]\n",
    "                start = 0\n",
    "                for num_points in points_per_polygon:\n",
    "                    if num_points == 0: continue\n",
    "                    polygon = transformed['keypoints'][start : start + num_points]\n",
    "                    transformed_polygons.append(np.array(polygon, dtype=np.float32))\n",
    "                    start += num_points\n",
    "                polygons = transformed_polygons\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation failed for {image_key}, using original. Error: {e}\")\n",
    "                polygons = [np.array(p['points'], dtype=np.float32) for p in polygons_data if 'points' in p]\n",
    "\n",
    "\n",
    "        gt_prob_map, gt_prob_mask, gt_thresh_map, gt_thresh_mask = self.make_db_ground_truth(polygons, image.shape[1], image.shape[2])\n",
    "\n",
    "        return image, gt_prob_map, gt_prob_mask, gt_thresh_map, gt_thresh_mask\n",
    "\n",
    "    def make_db_ground_truth(self, polygons, width, height):\n",
    "        gt_prob_map = np.zeros((height, width), dtype=np.float32)\n",
    "        gt_prob_mask = np.ones((height, width), dtype=np.float32)\n",
    "        gt_thresh_map = np.zeros((height, width), dtype=np.float32)\n",
    "        gt_thresh_mask = np.zeros((height, width), dtype=np.float32)\n",
    "        \n",
    "        for polygon in polygons:\n",
    "            if len(polygon) < 3: continue\n",
    "            polygon = np.clip(polygon, [0,0], [width-1, height-1])\n",
    "            \n",
    "            cv2.fillPoly(gt_prob_map, [polygon.astype(np.int32)], 1.0)\n",
    "            \n",
    "            pco = pyclipper.PyclipperOffset()\n",
    "            pco.AddPath(polygon, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "            \n",
    "            area = cv2.contourArea(polygon)\n",
    "            perimeter = cv2.arcLength(polygon, True)\n",
    "            if perimeter == 0: continue\n",
    "            distance = area * (1 - 0.4**2) / (perimeter + 1e-6)\n",
    "            \n",
    "            shrunk_polygons = pco.Execute(-distance)\n",
    "            if not shrunk_polygons: continue\n",
    "            \n",
    "            cv2.fillPoly(gt_thresh_mask, [np.array(p, dtype=np.int32) for p in shrunk_polygons], 1.0)\n",
    "            \n",
    "            dist_map = np.zeros((height, width), dtype=np.float32)\n",
    "            cv2.fillPoly(dist_map, [polygon.astype(np.int32)], 1.0)\n",
    "            dist_map = cv2.distanceTransform(dist_map.astype(np.uint8), cv2.DIST_L2, 5)\n",
    "            \n",
    "            min_dist, max_dist = np.min(dist_map), np.max(dist_map)\n",
    "            if max_dist > min_dist:\n",
    "                dist_map = (dist_map - min_dist) / (max_dist - min_dist)\n",
    "            \n",
    "            gt_thresh_map[dist_map > 0] = dist_map[dist_map > 0]\n",
    "\n",
    "        return torch.from_numpy(gt_prob_map).unsqueeze(0), torch.from_numpy(gt_prob_mask).unsqueeze(0), torch.from_numpy(gt_thresh_map).unsqueeze(0), torch.from_numpy(gt_thresh_mask).unsqueeze(0)\n",
    "def get_transforms(size):\n",
    "    # keypoint_params format='xy'는 (x, y) 좌표를 의미\n",
    "    return A.Compose([\n",
    "        A.Resize(size, size, always_apply=True),\n",
    "        A.Rotate(limit=10, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.ColorJitter(p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 2: 모델 및 손실 함수 아키텍처\n",
    "# ====================================================================================\n",
    "class MobileNetV3_Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        # mobilenet_v3_large의 특징 추출기 부분을 가져옵니다.\n",
    "        self.features = mobilenet_v3_large(weights='IMAGENET1K_V1' if pretrained else None).features\n",
    "        \n",
    "        # FPN으로 특징을 전달할 레이어의 인덱스를 지정합니다.\n",
    "        # 이 인덱스는 MobileNetV3-Large의 구조에 따릅니다.\n",
    "        self.output_indices = [3, 6, 12, 16]\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        # features 모듈을 순차적으로 통과시키며, 지정된 인덱스에서 출력을 저장합니다.\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if i in self.output_indices:\n",
    "                outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "class DB_FPN(nn.Module):\n",
    "    # MobileNetV3-Large의 실제 출력 채널에 맞게 기본값을 수정합니다.\n",
    "    def __init__(self, in_channels=[24, 40, 112, 960], out_channels=256):\n",
    "        super().__init__()\n",
    "        self.in_convs = nn.ModuleList([nn.Conv2d(c, out_channels, 1, bias=False) for c in in_channels])\n",
    "        self.out_convs = nn.ModuleList([nn.Conv2d(out_channels, out_channels // 4, 3, padding=1, bias=False) for _ in in_channels])\n",
    "\n",
    "    def forward(self, features):\n",
    "        # features의 순서는 [C2, C3, C4, C5] 입니다. (작은 것 -> 큰 것)\n",
    "        c2, c3, c4, c5 = features\n",
    "\n",
    "        # 입력 채널에 맞게 1x1 conv 적용\n",
    "        in5 = self.in_convs[3](c5)\n",
    "        in4 = self.in_convs[2](c4)\n",
    "        in3 = self.in_convs[1](c3)\n",
    "        in2 = self.in_convs[0](c2)\n",
    "\n",
    "        # Top-down 경로 (FPN의 핵심)\n",
    "        out4 = in4 + F.interpolate(in5, size=in4.shape[2:], mode='bilinear', align_corners=False) # P5 -> P4\n",
    "        out3 = in3 + F.interpolate(out4, size=in3.shape[2:], mode='bilinear', align_corners=False) # P4 -> P3\n",
    "        out2 = in2 + F.interpolate(out3, size=in2.shape[2:], mode='bilinear', align_corners=False) # P3 -> P2\n",
    "\n",
    "        # 3x3 conv로 최종 특징 맵 생성 및 업샘플링 후 합치기\n",
    "        p5 = F.interpolate(self.out_convs[3](in5), size=out2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        p4 = F.interpolate(self.out_convs[2](out4), size=out2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        p3 = F.interpolate(self.out_convs[1](out3), size=out2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        p2 = self.out_convs[0](out2)\n",
    "        \n",
    "        return torch.cat([p2, p3, p4, p5], dim=1)\n",
    "\n",
    "# DB_Head, DBNet, DiceLoss, DBLoss 클래스는 수정할 필요가 없습니다.\n",
    "# (기존 코드 그대로 사용)\n",
    "class DB_Head(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super().__init__()\n",
    "        # [수정] 최종 출력이 640x640이 되도록 nn.Upsample 레이어를 추가합니다.\n",
    "        self.prob_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 3, padding=1), \n",
    "            nn.BatchNorm2d(in_channels // 4), \n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(in_channels // 4, 1, 2, 2), # 여기까지 출력이 320x320\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) # 320x320 -> 640x640\n",
    "        )\n",
    "        self.thresh_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 3, padding=1), \n",
    "            nn.BatchNorm2d(in_channels // 4), \n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(in_channels // 4, 1, 2, 2), # 여기까지 출력이 320x320\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) # 320x320 -> 640x640\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return torch.sigmoid(self.prob_conv(x)), torch.sigmoid(self.thresh_conv(x))\n",
    "\n",
    "\n",
    "class DBNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = MobileNetV3_Backbone(pretrained)\n",
    "        self.fpn = DB_FPN()\n",
    "        self.head = DB_Head()\n",
    "    def forward(self, x): return self.head(self.fpn(self.backbone(x)))\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__(); self.eps = eps\n",
    "    def forward(self, pred, target, mask):\n",
    "        pred, target = pred * mask, target * mask; intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum() + self.eps; return 1 - (2 * intersection / union)\n",
    "\n",
    "class DBLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=10.0, k=50):\n",
    "        super().__init__(); self.alpha, self.beta, self.k = alpha, beta, k\n",
    "        self.bce_loss = nn.BCELoss(reduction='none'); self.dice_loss = DiceLoss()\n",
    "    def forward(self, pred_maps, gt_maps):\n",
    "        pred_prob, pred_thresh = pred_maps; gt_prob, gt_prob_mask, gt_thresh, gt_thresh_mask = gt_maps\n",
    "        loss_s = self.dice_loss(pred_prob, gt_prob, gt_prob_mask) + (self.bce_loss(pred_prob, gt_prob) * gt_prob_mask).mean()\n",
    "        db_map = 1 / (1 + torch.exp(-self.k * (pred_prob - pred_thresh)))\n",
    "        loss_b = self.dice_loss(db_map, gt_prob, gt_prob_mask)\n",
    "        loss_t = (torch.abs(pred_thresh - gt_thresh) * gt_thresh_mask).sum() / (gt_thresh_mask.sum() + 1e-6)\n",
    "        return loss_s + self.alpha * loss_b + self.beta * loss_t\n",
    "\n",
    "# ====================================================================================\n",
    "# Part 3: 실제 학습 스크립트\n",
    "# ====================================================================================\n",
    "def main():\n",
    "    # --- 1. 설정 (사용자 환경에 맞게 수정) ---\n",
    "    # 이 경로들을 자신의 데이터셋 경로로 반드시 수정해야 합니다.\n",
    "    ANNOTATION_PATH = r\"C:\\Users\\User\\DBNet_OCR\\[라벨]Training\\2.책표지\\04.사회과학\\combined_labels_cleaned.json\"\n",
    "    IMAGE_DIR = r\"C:\\Users\\User\\DBNet_OCR\\[원천]Training_책표지1\\04.사회과학\" \n",
    "    \n",
    "    # 하이퍼파라미터\n",
    "    IMG_SIZE = 640\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 20 # 실제 학습을 위해 Epoch 수를 늘립니다.\n",
    "    LEARNING_RATE = 1e-4\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(f\"사용 장치: {DEVICE}\")\n",
    "    print(\"사전 학습된 MobileNetV3 가중치를 사용합니다.\")\n",
    "\n",
    "    # --- 2. 데이터 파이프라인 준비 ---\n",
    "    print(\"데이터 로더를 준비합니다...\")\n",
    "    try:\n",
    "        dataset = OcrDataset(\n",
    "            annotation_path=ANNOTATION_PATH,\n",
    "            image_dir=IMAGE_DIR,\n",
    "            transform=get_transforms(IMG_SIZE)\n",
    "        )\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers = 0  # CPU 코어 수에 맞게 조절\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"오류: 데이터셋 경로를 찾을 수 없습니다. ANNOTATION_PATH와 IMAGE_DIR을 확인하세요.\")\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # --- 3. 모델, 손실함수, 옵티마이저 준비 ---\n",
    "    print(\"모델과 옵티마이저를 준비합니다...\")\n",
    "    detector = DBNet(pretrained=True).to(DEVICE)\n",
    "    db_loss_fn = DBLoss().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(detector.parameters(), lr=LEARNING_RATE) # AdamW가 종종 더 나은 성능을 보임\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(data_loader) * NUM_EPOCHS)\n",
    "\n",
    "    # --- 4. 학습 루프 ---\n",
    "    print(\"학습을 시작합니다...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        detector.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for batch_idx, (images, gt_prob, gt_prob_mask, gt_thresh, gt_thresh_mask) in enumerate(progress_bar):\n",
    "            images = images.to(DEVICE)\n",
    "            gt_prob, gt_prob_mask = gt_prob.to(DEVICE), gt_prob_mask.to(DEVICE)\n",
    "            gt_thresh, gt_thresh_mask = gt_thresh.to(DEVICE), gt_thresh_mask.to(DEVICE)\n",
    "            \n",
    "            pred_prob, pred_thresh = detector(images)\n",
    "            loss = db_loss_fn((pred_prob, pred_thresh), (gt_prob, gt_prob_mask, gt_thresh, gt_thresh_mask))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() # 매 스텝마다 학습률 조절\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{scheduler.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch+1} 완료, 평균 손실: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- 5. 체크포인트 저장 ---\n",
    "        if (epoch + 1) % 5 == 0: # 5 에포크마다 저장\n",
    "            torch.save(detector.state_dict(), f\"dbnet_detector_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Epoch {epoch+1} 모델 저장 완료.\")\n",
    "\n",
    "    print(\"학습이 모두 완료되었습니다.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7870929-44ab-41c0-8d3f-717fc4cb0412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.10 (Py3.10)",
   "language": "python",
   "name": "tensortf210py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
