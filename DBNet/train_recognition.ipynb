{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf15da67-d0f6-483a-b11f-5a9bbec4f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\n",
      "==================================================\n",
      "CHARACTER_SET 생성이 완료되었습니다.\n",
      "총 글자 수: 1550\n",
      "==================================================\n",
      "\n",
      ">> 2. 모델 학습을 시작합니다...\n",
      "===== 텍스트 인식 모델 학습 시작 =====\n",
      "데이터 로더를 준비합니다...\n",
      "인식 모델의 클래스 개수 (blank 포함): 1551\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00002547.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00007143.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00012858.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00044170.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00097253.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00123738.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00127828.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00208717.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00249756.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4144\\3080662519.py:144: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(height, width, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00329351.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00345028.png\n",
      "모델과 옵티마이저를 준비합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|███████████████████████████████████████████████████| 12175/12175 [53:07<00:00,  3.82it/s, loss=0.8309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 완료, 평균 손실: 3.7081\n",
      "--- 검증 예시 ---\n",
      "  GT: '37                  ' | PRED: 'o'\n",
      "  GT: '40                  ' | PRED: 'R'\n",
      "  GT: '327                 ' | PRED: '비는의'\n",
      "  GT: '748                 ' | PRED: 'G'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|███████████████████████████████████████████████████| 12175/12175 [12:53<00:00, 15.73it/s, loss=1.0568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 완료, 평균 손실: 1.3746\n",
      "--- 검증 예시 ---\n",
      "  GT: '1008                ' | PRED: '와'\n",
      "  GT: '56                  ' | PRED: 'e'\n",
      "  GT: '1457                ' | PRED: '현'\n",
      "  GT: '1202                ' | PRED: '처'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|███████████████████████████████████████████████████| 12175/12175 [13:12<00:00, 15.37it/s, loss=0.6848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 완료, 평균 손실: 0.9754\n",
      "--- 검증 예시 ---\n",
      "  GT: '519                 ' | PRED: '러'\n",
      "  GT: '8                   ' | PRED: ','\n",
      "  GT: '1331                ' | PRED: '별'\n",
      "  GT: '964                 ' | PRED: '언'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|███████████████████████████████████████████████████| 12175/12175 [13:11<00:00, 15.38it/s, loss=0.0175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 완료, 평균 손실: 0.7954\n",
      "--- 검증 예시 ---\n",
      "  GT: '340                 ' | PRED: '누'\n",
      "  GT: '981                 ' | PRED: '요'\n",
      "  GT: '1064                ' | PRED: '일'\n",
      "  GT: '931                 ' | PRED: '아'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|███████████████████████████████████████████████████| 12175/12175 [12:56<00:00, 15.68it/s, loss=0.8192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 완료, 평균 손실: 0.6871\n",
      "--- 검증 예시 ---\n",
      "  GT: '66                  ' | PRED: 'O'\n",
      "  GT: '1217                ' | PRED: '조'\n",
      "  GT: '24                  ' | PRED: 'B'\n",
      "  GT: '414                 ' | PRED: '동'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|███████████████████████████████████████████████████| 12175/12175 [12:59<00:00, 15.62it/s, loss=0.3866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 완료, 평균 손실: 0.6074\n",
      "--- 검증 예시 ---\n",
      "  GT: '169                 ' | PRED: '과'\n",
      "  GT: '649                 ' | PRED: '문'\n",
      "  GT: '1056                ' | PRED: '음'\n",
      "  GT: '1061                ' | PRED: '이'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|███████████████████████████████████████████████████| 12175/12175 [12:54<00:00, 15.72it/s, loss=0.4219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 완료, 평균 손실: 0.5455\n",
      "--- 검증 예시 ---\n",
      "  GT: '872                 ' | PRED: '스'\n",
      "  GT: '752                 ' | PRED: '방'\n",
      "  GT: '1061                ' | PRED: '이'\n",
      "  GT: '468                 ' | PRED: '떤'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|███████████████████████████████████████████████████| 12175/12175 [13:06<00:00, 15.49it/s, loss=0.0183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 완료, 평균 손실: 0.4993\n",
      "--- 검증 예시 ---\n",
      "  GT: '649                 ' | PRED: '문'\n",
      "  GT: '1053                ' | PRED: '은빛나래가'\n",
      "  GT: '754                 ' | PRED: '필'\n",
      "  GT: '280                 ' | PRED: '긴'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|███████████████████████████████████████████████████| 12175/12175 [13:00<00:00, 15.60it/s, loss=0.0266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 완료, 평균 손실: 0.4565\n",
      "--- 검증 예시 ---\n",
      "  GT: '173                 ' | PRED: '광물과'\n",
      "  GT: '651                 ' | PRED: '문'\n",
      "  GT: '169                 ' | PRED: '과'\n",
      "  GT: '649                 ' | PRED: '귀'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:21<00:00, 15.19it/s, loss=0.0067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 완료, 평균 손실: 0.4226\n",
      "--- 검증 예시 ---\n",
      "  GT: '14                  ' | PRED: '3대'\n",
      "  GT: '380                 ' | PRED: '등'\n",
      "  GT: '441                 ' | PRED: '가'\n",
      "  GT: '108                 ' | PRED: '산'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:42<00:00, 14.81it/s, loss=0.0165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 완료, 평균 손실: 0.3917\n",
      "--- 검증 예시 ---\n",
      "  GT: '962                 ' | PRED: '의'\n",
      "  GT: '575                 ' | PRED: '외'\n",
      "  GT: '1061                ' | PRED: '케'\n",
      "  GT: '1015                ' | PRED: '특'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:27<00:00, 15.08it/s, loss=0.0870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 완료, 평균 손실: 0.3648\n",
      "--- 검증 예시 ---\n",
      "  GT: '1377                ' | PRED: '팅'\n",
      "  GT: '1060                ' | PRED: '의'\n",
      "  GT: '108                 ' | PRED: '가고'\n",
      "  GT: '159                 ' | PRED: '을'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:34<00:00, 14.95it/s, loss=1.3526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 완료, 평균 손실: 0.3401\n",
      "--- 검증 예시 ---\n",
      "  GT: '872                 ' | PRED: '스'\n",
      "  GT: '180                 ' | PRED: '국'\n",
      "  GT: '796                 ' | PRED: '상상'\n",
      "  GT: '796                 ' | PRED: '0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:29<00:00, 15.03it/s, loss=0.0071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 완료, 평균 손실: 0.3188\n",
      "--- 검증 예시 ---\n",
      "  GT: '1082                ' | PRED: '장군'\n",
      "  GT: '181                 ' | PRED: '관'\n",
      "  GT: '171                 ' | PRED: '이'\n",
      "  GT: '1061                ' | PRED: '음'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:23<00:00, 15.16it/s, loss=0.0533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 완료, 평균 손실: 0.2978\n",
      "--- 검증 예시 ---\n",
      "  GT: '1009                ' | PRED: '완'\n",
      "  GT: '66                  ' | PRED: 'O'\n",
      "  GT: '1119                ' | PRED: '주'\n",
      "  GT: '1224                ' | PRED: '최신개정판'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████████████████████████████████████████████| 12175/12175 [12:39<00:00, 16.04it/s, loss=0.0080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 완료, 평균 손실: 0.2803\n",
      "--- 검증 예시 ---\n",
      "  GT: '1061                ' | PRED: '이'\n",
      "  GT: '1438                ' | PRED: '한'\n",
      "  GT: '1119                ' | PRED: '주니어'\n",
      "  GT: '358                 ' | PRED: '비'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████████████████████████████████████████████| 12175/12175 [10:58<00:00, 18.49it/s, loss=0.6024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 완료, 평균 손실: 0.2642\n",
      "--- 검증 예시 ---\n",
      "  GT: '169                 ' | PRED: '과'\n",
      "  GT: '112                 ' | PRED: '갈'\n",
      "  GT: '506                 ' | PRED: '랑'\n",
      "  GT: '340                 ' | PRED: '누가'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:30<00:00, 15.02it/s, loss=0.0317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 완료, 평균 손실: 0.2494\n",
      "--- 검증 예시 ---\n",
      "  GT: '625                 ' | PRED: '멘'\n",
      "  GT: '546                 ' | PRED: '로'\n",
      "  GT: '601                 ' | PRED: '맛'\n",
      "  GT: '40                  ' | PRED: 'R'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:29<00:00, 15.04it/s, loss=1.2558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 완료, 평균 손실: 0.2362\n",
      "--- 검증 예시 ---\n",
      "  GT: '715                 ' | PRED: '병'\n",
      "  GT: '208                 ' | PRED: '기'\n",
      "  GT: '952                 ' | PRED: '야'\n",
      "  GT: '1060                ' | PRED: '의'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:40<00:00, 14.84it/s, loss=0.2917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 완료, 평균 손실: 0.2242\n",
      "--- 검증 예시 ---\n",
      "  GT: '646                 ' | PRED: '무'\n",
      "  GT: '40                  ' | PRED: 'hOBIN'\n",
      "  GT: '37                  ' | PRED: '면의'\n",
      "  GT: '24                  ' | PRED: '실'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:18<00:00, 15.25it/s, loss=0.0027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 완료, 평균 손실: 0.2123\n",
      "--- 검증 예시 ---\n",
      "  GT: '687                 ' | PRED: '공'\n",
      "  GT: '964                 ' | PRED: '늘'\n",
      "  GT: '352                 ' | PRED: '효'\n",
      "  GT: '1500                ' | PRED: 'n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:25<00:00, 15.12it/s, loss=0.0076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 완료, 평균 손실: 0.2015\n",
      "--- 검증 예시 ---\n",
      "  GT: '1195                ' | PRED: '책'\n",
      "  GT: '19                  ' | PRED: '8'\n",
      "  GT: '179                 ' | PRED: '구'\n",
      "  GT: '1060                ' | PRED: '의'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:27<00:00, 15.07it/s, loss=0.0035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 완료, 평균 손실: 0.1915\n",
      "--- 검증 예시 ---\n",
      "  GT: '498                 ' | PRED: '라'\n",
      "  GT: '812                 ' | PRED: '서'\n",
      "  GT: '1411                ' | PRED: '폴'\n",
      "  GT: '676                 ' | PRED: '바'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████████████████████████████████████████████| 12175/12175 [11:29<00:00, 17.66it/s, loss=1.2739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 완료, 평균 손실: 0.1810\n",
      "--- 검증 예시 ---\n",
      "  GT: '1480                ' | PRED: '혜'\n",
      "  GT: '1067                ' | PRED: '임연기'\n",
      "  GT: '984                 ' | PRED: 'EDIION'\n",
      "  GT: '208                 ' | PRED: '우'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████████████████████████████████████████████| 12175/12175 [12:23<00:00, 16.38it/s, loss=0.0424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 완료, 평균 손실: 0.1724\n",
      "--- 검증 예시 ---\n",
      "  GT: '118                 ' | PRED: '강'\n",
      "  GT: '546                 ' | PRED: '로'\n",
      "  GT: '1091                ' | PRED: '전'\n",
      "  GT: '66                  ' | PRED: 'o'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:20<00:00, 15.21it/s, loss=0.0603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 완료, 평균 손실: 0.1644\n",
      "--- 검증 예시 ---\n",
      "  GT: '872                 ' | PRED: '스프링북'\n",
      "  GT: '1425                ' | PRED: 'k'\n",
      "  GT: '591                 ' | PRED: '함께'\n",
      "  GT: '731                 ' | PRED: '새'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:20<00:00, 15.22it/s, loss=0.0191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 완료, 평균 손실: 0.1565\n",
      "--- 검증 예시 ---\n",
      "  GT: '110                 ' | PRED: '간'\n",
      "  GT: '812                 ' | PRED: '서'\n",
      "  GT: '1437                ' | PRED: '학'\n",
      "  GT: '983                 ' | PRED: '엮'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:06<00:00, 15.47it/s, loss=0.0771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 완료, 평균 손실: 0.1491\n",
      "--- 검증 예시 ---\n",
      "  GT: '441                 ' | PRED: '등'\n",
      "  GT: '328                 ' | PRED: '녹'\n",
      "  GT: '1431                ' | PRED: '핀'\n",
      "  GT: '940                 ' | PRED: '앗'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:24<00:00, 15.13it/s, loss=0.1050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 완료, 평균 손실: 0.1424\n",
      "--- 검증 예시 ---\n",
      "  GT: '1126                ' | PRED: '중'\n",
      "  GT: '1266                ' | PRED: '케'\n",
      "  GT: '1056                ' | PRED: '음'\n",
      "  GT: '1095                ' | PRED: '점'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:19<00:00, 15.22it/s, loss=0.0075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 완료, 평균 손실: 0.1366\n",
      "--- 검증 예시 ---\n",
      "  GT: '1095                ' | PRED: '점'\n",
      "  GT: '164                 ' | PRED: '곰'\n",
      "  GT: '1217                ' | PRED: '초'\n",
      "  GT: '40                  ' | PRED: 'R'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:22<00:00, 15.17it/s, loss=1.7588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 완료, 평균 손실: 0.1302\n",
      "--- 검증 예시 ---\n",
      "  GT: '108                 ' | PRED: '가'\n",
      "  GT: '59                  ' | PRED: 'h'\n",
      "  GT: '57                  ' | PRED: 'f'\n",
      "  GT: '72                  ' | PRED: 'u'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:13<00:00, 15.34it/s, loss=0.1807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 완료, 평균 손실: 0.1238\n",
      "--- 검증 예시 ---\n",
      "  GT: '617                 ' | PRED: '먹'\n",
      "  GT: '676                 ' | PRED: '바'\n",
      "  GT: '1063                ' | PRED: '인성을'\n",
      "  GT: '820                 ' | PRED: '사'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:17<00:00, 15.27it/s, loss=0.0421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 완료, 평균 손실: 0.1189\n",
      "--- 검증 예시 ---\n",
      "  GT: '1097                ' | PRED: '정'\n",
      "  GT: '17                  ' | PRED: '6'\n",
      "  GT: '189                 ' | PRED: '권'\n",
      "  GT: '56                  ' | PRED: 'e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:25<00:00, 15.12it/s, loss=0.0377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 완료, 평균 손실: 0.1132\n",
      "--- 검증 예시 ---\n",
      "  GT: '593                 ' | PRED: '막'\n",
      "  GT: '690                 ' | PRED: '백구'\n",
      "  GT: '179                 ' | PRED: '나'\n",
      "  GT: '280                 ' | PRED: '그림'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:11<00:00, 15.39it/s, loss=0.0035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 완료, 평균 손실: 0.1084\n",
      "--- 검증 예시 ---\n",
      "  GT: '616                 ' | PRED: '머'\n",
      "  GT: '1097                ' | PRED: '정신과'\n",
      "  GT: '883                 ' | PRED: '물고기'\n",
      "  GT: '169                 ' | PRED: '의'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:19<00:00, 15.23it/s, loss=0.5389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 완료, 평균 손실: 0.1049\n",
      "--- 검증 예시 ---\n",
      "  GT: '884                 ' | PRED: '실력이'\n",
      "  GT: '538                 ' | PRED: '드'\n",
      "  GT: '1061                ' | PRED: '신'\n",
      "  GT: '432                 ' | PRED: '줘'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:24<00:00, 15.14it/s, loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 완료, 평균 손실: 0.0999\n",
      "--- 검증 예시 ---\n",
      "  GT: '31                  ' | PRED: 'I'\n",
      "  GT: '1091                ' | PRED: '전'\n",
      "  GT: '796                 ' | PRED: '상'\n",
      "  GT: '1257                ' | PRED: '커피'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:17<00:00, 15.26it/s, loss=0.4542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 완료, 평균 손실: 0.0955\n",
      "--- 검증 예시 ---\n",
      "  GT: '1438                ' | PRED: '한'\n",
      "  GT: '1061                ' | PRED: '이'\n",
      "  GT: '34                  ' | PRED: 'L'\n",
      "  GT: '41                  ' | PRED: 'S'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:25<00:00, 15.11it/s, loss=0.0028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 완료, 평균 손실: 0.0910\n",
      "--- 검증 예시 ---\n",
      "  GT: '1500                ' | PRED: '효'\n",
      "  GT: '1336                ' | PRED: '테'\n",
      "  GT: '208                 ' | PRED: '기'\n",
      "  GT: '41                  ' | PRED: 'S'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:18<00:00, 15.25it/s, loss=0.3566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 완료, 평균 손실: 0.0887\n",
      "--- 검증 예시 ---\n",
      "  GT: '1126                ' | PRED: '중국문자학의'\n",
      "  GT: '180                 ' | PRED: '락'\n",
      "  GT: '649                 ' | PRED: '수습차제'\n",
      "  GT: '1074                ' | PRED: 'e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:23<00:00, 15.16it/s, loss=0.0014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 완료, 평균 손실: 0.0845\n",
      "--- 검증 예시 ---\n",
      "  GT: '1437                ' | PRED: '학'\n",
      "  GT: '66                  ' | PRED: 'o'\n",
      "  GT: '26                  ' | PRED: 'D'\n",
      "  GT: '812                 ' | PRED: '서'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:19<00:00, 15.22it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 완료, 평균 손실: 0.0807\n",
      "--- 검증 예시 ---\n",
      "  GT: '1027                ' | PRED: '움'\n",
      "  GT: '1474                ' | PRED: '현'\n",
      "  GT: '1140                ' | PRED: '니'\n",
      "  GT: '802                 ' | PRED: '생각하는'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████████████████████████████████████████████| 12175/12175 [11:22<00:00, 17.84it/s, loss=0.0004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 완료, 평균 손실: 0.0787\n",
      "--- 검증 예시 ---\n",
      "  GT: '199                 ' | PRED: '그림'\n",
      "  GT: '588                 ' | PRED: '영문법'\n",
      "  GT: '990                 ' | PRED: '그'\n",
      "  GT: '649                 ' | PRED: '나는'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████████████████████████████████████████████| 12175/12175 [12:46<00:00, 15.89it/s, loss=0.0099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 완료, 평균 손실: 0.0759\n",
      "--- 검증 예시 ---\n",
      "  GT: '1404                ' | PRED: '편'\n",
      "  GT: '37                  ' | PRED: 'O'\n",
      "  GT: '180                 ' | PRED: '국'\n",
      "  GT: '820                 ' | PRED: '성'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:21<00:00, 15.19it/s, loss=0.0513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 완료, 평균 손실: 0.0711\n",
      "--- 검증 예시 ---\n",
      "  GT: '718                 ' | PRED: '복'\n",
      "  GT: '920                 ' | PRED: '쓰'\n",
      "  GT: '1107                ' | PRED: '조'\n",
      "  GT: '1064                ' | PRED: '일'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:11<00:00, 15.37it/s, loss=0.0095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 완료, 평균 손실: 0.0677\n",
      "--- 검증 예시 ---\n",
      "  GT: '406                 ' | PRED: '도'\n",
      "  GT: '863                 ' | PRED: '쉬'\n",
      "  GT: '1204                ' | PRED: '천'\n",
      "  GT: '26                  ' | PRED: 'D'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████████████████████████████████████████████| 12175/12175 [12:49<00:00, 15.82it/s, loss=0.5922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 완료, 평균 손실: 0.0658\n",
      "--- 검증 예시 ---\n",
      "  GT: '123                 ' | PRED: '개'\n",
      "  GT: '1053                ' | PRED: '은'\n",
      "  GT: '1004                ' | PRED: '옳'\n",
      "  GT: '999                 ' | PRED: '오프라'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████████████████████████████████████████████| 12175/12175 [12:10<00:00, 16.67it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 완료, 평균 손실: 0.0636\n",
      "--- 검증 예시 ---\n",
      "  GT: '14                  ' | PRED: '33가지'\n",
      "  GT: '14                  ' | PRED: '관'\n",
      "  GT: '108                 ' | PRED: '른'\n",
      "  GT: '1140                ' | PRED: '에'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:30<00:00, 15.02it/s, loss=0.5289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 완료, 평균 손실: 0.0603\n",
      "--- 검증 예시 ---\n",
      "  GT: '133                 ' | PRED: '거'\n",
      "  GT: '169                 ' | PRED: '과'\n",
      "  GT: '1365                ' | PRED: '트'\n",
      "  GT: '1379                ' | PRED: '판'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████████████████████████████████████████████| 12175/12175 [13:21<00:00, 15.20it/s, loss=0.0076]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4144\\128659728.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 완료, 평균 손실: 0.0576\n",
      "--- 검증 예시 ---\n",
      "  GT: '436                 ' | PRED: '들'\n",
      "  GT: '63                  ' | PRED: 'lonely'\n",
      "  GT: '66                  ' | PRED: '하'\n",
      "  GT: '65                  ' | PRED: '영'\n",
      "\n",
      "학습 완료! 모델이 'crnn_recognizer_final.pth'로 저장되었습니다.\n",
      "\n",
      ">> 3. 추론 테스트를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4144\\3080662519.py:144: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(height, width, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 테스트 결과 ---\n",
      "이미지: C:\\Users\\User\\DBNet_OCR\\data\\crop\\images\\rec_crop_00000000.png\n",
      "예측된 텍스트: '우'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================================\n",
    "# 0. 기본 설정 및 라이브러리 임포트\n",
    "# ====================================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "# 학습에 사용할 문자셋. 실제 데이터셋에 맞게 수정해야 합니다.\n",
    "GT_FILE_PATH = r\"DBNet_OCR/data/crop/gt.txt\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def generate_character_set(gt_file):\n",
    "    if not os.path.exists(gt_file):\n",
    "        print(f\"오류: gt.txt 파일을 찾을 수 없습니다! 경로를 확인하세요: {gt_file}\")\n",
    "        return None # 오류 발생 시 None 반환\n",
    "\n",
    "    all_characters = set()\n",
    "    with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                _, text = line.strip().split('\\t', 1)\n",
    "                for char in text:\n",
    "                    all_characters.add(char)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    sorted_characters = sorted(list(all_characters))\n",
    "    final_charset = \"\".join(sorted_characters)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"CHARACTER_SET 생성이 완료되었습니다.\")\n",
    "    print(f\"총 글자 수: {len(final_charset)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return final_charset # 생성된 문자열을 반환\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# 1. 레이블 변환기 (CTCLabelConverter)\n",
    "# - 역할: 텍스트 문자열을 모델이 이해할 수 있는 숫자 시퀀스로, \n",
    "#         모델의 출력(숫자 시퀀스)을 다시 텍스트 문자열로 변환합니다.\n",
    "# ====================================================================================\n",
    "class CTCLabelConverter:\n",
    "    \"\"\"텍스트와 인덱스 간의 변환을 담당하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, character_set):\n",
    "        # 0번 인덱스는 CTC Loss를 위한 'blank' 토큰으로 예약합니다.\n",
    "        self.character_set = [\"-\"] + list(character_set)\n",
    "        \n",
    "        # 문자를 인덱스로 변환하는 딕셔너리\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.character_set)}\n",
    "        # 인덱스를 문자로 변환하는 딕셔너리\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.character_set)}\n",
    "        \n",
    "        print(f\"인식 모델의 클래스 개수 (blank 포함): {self.get_num_classes()}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"입력된 텍스트 문자열을 숫자 인덱스의 리스트로 변환합니다.\"\"\"\n",
    "        # character_set에 없는 문자는 무시합니다.\n",
    "        indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"모델의 출력(인덱스 시퀀스)을 텍스트 문자열로 디코딩합니다.\"\"\"\n",
    "        # CTC Greedy Decode 방식:\n",
    "        # 1. 가장 확률이 높은 인덱스를 선택합니다.\n",
    "        # 2. 연속되는 중복 인덱스를 제거합니다.\n",
    "        # 3. blank(인덱스 0) 토큰을 제거합니다.\n",
    "        text = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx_item = idx.item()\n",
    "            if idx_item == 0:  # blank 토큰은 무시\n",
    "                last_idx = 0\n",
    "                continue\n",
    "            if idx_item == last_idx:  # 연속 중복 문자 무시\n",
    "                continue\n",
    "            \n",
    "            text.append(self.idx_to_char[idx_item])\n",
    "            last_idx = idx_item\n",
    "            \n",
    "        return \"\".join(text)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        \"\"\"blank 토큰을 포함한 전체 클래스의 개수를 반환합니다.\"\"\"\n",
    "        return len(self.character_set)\n",
    "\n",
    "# ====================================================================================\n",
    "# 2. 데이터 파이프라인 (Dataset 및 DataLoader)\n",
    "# - 역할: 디스크에 저장된 이미지와 텍스트 파일을 불러와 모델 학습에 사용할 수 있는\n",
    "#         PyTorch 텐서(Tensor) 형태로 변환하고, 배치(batch) 단위로 묶어줍니다.\n",
    "# ====================================================================================\n",
    "\n",
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"인식용 데이터셋을 위한 클래스\"\"\"\n",
    "    def __init__(self, gt_file_path, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # gt.txt 파일을 읽어 (이미지 파일명, 텍스트) 쌍을 리스트에 저장합니다.\n",
    "        with open(gt_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    filename, text = line.strip().split('\\t')\n",
    "                    self.samples.append((filename, text))\n",
    "                except ValueError:\n",
    "                    # 탭으로 분리되지 않은 줄은 건너뜁니다.\n",
    "                    print(f\"경고: 잘못된 형식의 라인 발견 - {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        # 전체 데이터셋의 샘플 수를 반환합니다.\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 샘플을 반환합니다.\n",
    "        filename, text = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        # OpenCV로 이미지를 읽습니다.\n",
    "        image = cv2.imread(image_path)\n",
    "        # BGR을 RGB로 변환합니다.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 데이터 증강 및 텐서 변환을 적용합니다.\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "            \n",
    "        return image, text\n",
    "\n",
    "def get_recognition_transforms(height, width):\n",
    "    \"\"\"인식 모델 학습을 위한 데이터 증강 파이프라인을 정의합니다.\"\"\"\n",
    "    return A.Compose([\n",
    "        # CRNN은 입력 이미지의 높이가 고정되어야 하므로, 항상 리사이즈합니다.\n",
    "        A.Resize(height, width, always_apply=True),\n",
    "        # 밝기, 대비 등 색상 관련 증강을 적용합니다.\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "        # 이미지를 -1에서 1 범위로 정규화합니다.\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        # PyTorch 텐서로 변환합니다.\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def recognition_collate_fn(batch, converter):\n",
    "    \"\"\"\n",
    "    가변 길이의 텍스트 레이블을 처리하기 위한 커스텀 collate 함수.\n",
    "    DataLoader가 이 함수를 사용해 여러 샘플을 하나의 배치로 만듭니다.\n",
    "    \"\"\"\n",
    "    images, texts = zip(*batch)\n",
    "    # 이미지들은 크기가 같으므로 stack을 이용해 하나의 텐서로 합칩니다.\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    # 텍스트들을 인코딩합니다.\n",
    "    encoded_texts = [converter.encode(text) for text in texts]\n",
    "    # 각 텍스트의 길이를 저장합니다.\n",
    "    target_lengths = torch.tensor([len(t) for t in encoded_texts], dtype=torch.long)\n",
    "    # 모든 텍스트의 인덱스를 하나의 1D 텐서로 이어붙입니다.\n",
    "    targets = torch.cat(encoded_texts)\n",
    "    \n",
    "    return images, targets, target_lengths\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# 3. 텍스트 인식 모델 (CRNN) 아키텍처\n",
    "# - 역할: (CNN) 이미지에서 특징 추출 -> (RNN) 특징의 순서와 문맥 학습 -> (Classifier) 글자 예측\n",
    "# ====================================================================================\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars, rnn_hidden_size=256, rnn_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. CNN 특징 추출기 (VGG 스타일) ---\n",
    "        # 입력 이미지: (B, 3, 32, 100)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 64, 16, 50)\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 128, 8, 25)\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 256, 4, 25)\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 512, 2, 25)\n",
    "            nn.Conv2d(512, 512, (2,1), 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)  # -> (B, 512, 1, 25)\n",
    "        )\n",
    "        \n",
    "        # --- 2. RNN (LSTM) 문맥 학습기 ---\n",
    "        # CNN 출력 특징맵을 RNN이 처리할 수 있는 시퀀스 형태로 변환\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512,          # CNN 출력의 채널 수\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=rnn_layers,\n",
    "            bidirectional=True,      # 양방향 RNN으로 더 넓은 문맥 파악\n",
    "            dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier (분류기) ---\n",
    "        # RNN의 출력을 각 문자의 확률로 변환\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_chars) # 양방향이므로 *2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. CNN을 통과시켜 이미지 특징 추출\n",
    "        features = self.cnn(x)  # -> (Batch, Channels, Height, Width) = (B, 512, 1, 25)\n",
    "        \n",
    "        # 2. RNN 입력 형식으로 변환: (SeqLen, Batch, InputSize)\n",
    "        b, c, h, w = features.size()\n",
    "        assert h == 1, \"CNN 출력의 높이는 1이어야 합니다.\"\n",
    "        features = features.squeeze(2)      # 높이(H) 차원 제거 -> (B, 512, 25)\n",
    "        features = features.permute(2, 0, 1)  # 차원 순서 변경 -> (W, B, C) = (25, B, 512)\n",
    "        \n",
    "        # 3. RNN을 통과시켜 문맥 정보 학습\n",
    "        rnn_output, _ = self.rnn(features) # -> (SeqLen, Batch, HiddenSize*2)\n",
    "        \n",
    "        # 4. 각 시퀀스 스텝에 대해 문자 분류\n",
    "        output = self.classifier(rnn_output) # -> (SeqLen, Batch, NumClasses)\n",
    "        return output\n",
    "\n",
    "# ====================================================================================\n",
    "# 4. 모델 학습 스크립트\n",
    "# ====================================================================================\n",
    "def train_recognizer(character_set, gt_file_path, image_dir):\n",
    "    print(\"===== 텍스트 인식 모델 학습 시작 =====\")\n",
    "    \n",
    "    # --- 2. 설정 ---\n",
    "    IMG_HEIGHT = 32\n",
    "    IMG_WIDTH = 100\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.1\n",
    "    \n",
    "    # --- 3. 데이터 파이프라인 준비 ---\n",
    "    print(\"데이터 로더를 준비합니다...\")\n",
    "    converter = CTCLabelConverter(character_set)\n",
    "    NUM_CLASSES = converter.get_num_classes()\n",
    "    \n",
    "    dataset = RecognitionDataset(\n",
    "        gt_file_path=gt_file_path,\n",
    "        image_dir=image_dir,\n",
    "        transform=get_recognition_transforms(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"오류: 데이터셋에 샘플이 없습니다. 경로를 확인해주세요.\")\n",
    "        return None, None\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0, # Windows 환경에서는 0이 안정적\n",
    "        collate_fn=lambda batch: recognition_collate_fn(batch, converter)\n",
    "    )\n",
    "\n",
    "    # --- 4. 모델, 손실함수, 옵티마이저 준비 ---\n",
    "    print(\"모델과 옵티마이저를 준비합니다...\")\n",
    "    model = CRNN(num_chars=NUM_CLASSES).to(DEVICE)\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 5. 학습 루프 ---\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for images, targets, target_lengths in progress_bar:\n",
    "            images, targets, target_lengths = images.to(DEVICE), targets.to(DEVICE), target_lengths.to(DEVICE)\n",
    "            \n",
    "            preds = model(images)\n",
    "            log_probs = F.log_softmax(preds, dim=2)\n",
    "            input_lengths = torch.full(size=(images.size(0),), fill_value=preds.size(0), dtype=torch.long)\n",
    "            \n",
    "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(data_loader)\n",
    "        print(f\"\\nEpoch {epoch+1} 완료, 평균 손실: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- 6. 간단한 검증 ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # next(iter())는 데이터로더에서 딱 한 배치만 가져옵니다.\n",
    "                val_images, val_texts, _ = next(iter(data_loader)) # target_lengths는 사용하지 않으므로 _로 받음\n",
    "                val_preds = model(val_images.to(DEVICE)).argmax(2).permute(1, 0)\n",
    "                \n",
    "                print(\"--- 검증 예시 ---\")\n",
    "                for i in range(min(4, len(val_texts))):\n",
    "                    decoded_text = converter.decode(val_preds[i])\n",
    "                    print(f\"  GT: '{val_texts[i]:<20}' | PRED: '{decoded_text}'\")\n",
    "            except StopIteration:\n",
    "                print(\"검증을 위한 데이터가 부족합니다.\")\n",
    "\n",
    "    # --- 7. 학습된 모델 저장 ---\n",
    "    torch.save(model.state_dict(), \"crnn_recognizer_final.pth\")\n",
    "    print(\"\\n학습 완료! 모델이 'crnn_recognizer_final.pth'로 저장되었습니다.\")\n",
    "    return \"crnn_recognizer_final.pth\", converter\n",
    "\n",
    "# ====================================================================================\n",
    "# 5. 학습된 모델을 사용한 추론(Inference) 모듈\n",
    "# - 역할: 학습이 끝난 모델 가중치(.pth)를 불러와 새로운 이미지의 글자를 읽습니다.\n",
    "# ====================================================================================\n",
    "# ====================================================================================\n",
    "# 메인 실행 블록 (최종 수정본)\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- [1단계: 설정] 학습할 데이터의 경로를 여기에 정의합니다. ---\n",
    "    GT_FILE_PATH = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\gt.txt\" # 본인의 gt.txt 경로\n",
    "    IMAGE_DIR = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\images\"   # 본인의 images 폴더 경로\n",
    "\n",
    "    # --- [2단계: 문자셋 생성] 위 경로의 gt.txt 파일로부터 학습에 필요한 모든 글자를 추출합니다. ---\n",
    "    print(\">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\")\n",
    "    charset = generate_character_set(GT_FILE_PATH)\n",
    "    \n",
    "    # --- [3단계: 학습 시작] 문자셋이 성공적으로 생성되면, 이 정보들을 가지고 학습을 시작합니다. ---\n",
    "    if charset:\n",
    "        print(\"\\n>> 2. 모델 학습을 시작합니다...\")\n",
    "        # train_recognizer 함수에 필요한 모든 정보(charset, gt_file_path, image_dir)를 인자로 전달합니다.\n",
    "        trained_model_path, label_converter = train_recognizer(charset, GT_FILE_PATH, IMAGE_DIR)\n",
    "        \n",
    "        # --- [4단계: 추론 테스트] 학습이 성공적으로 끝나면, 결과 모델로 테스트를 진행합니다. ---\n",
    "        if trained_model_path and label_converter:\n",
    "            print(\"\\n>> 3. 추론 테스트를 시작합니다...\")\n",
    "            recognizer = Recognizer(trained_model_path, label_converter, DEVICE)\n",
    "            \n",
    "            try:\n",
    "                # 테스트할 이미지 선택 (학습 데이터셋의 첫 번째 이미지로 테스트)\n",
    "                test_image_name = os.listdir(IMAGE_DIR)[0]\n",
    "                test_image_path = os.path.join(IMAGE_DIR, test_image_name)\n",
    "                \n",
    "                # 예측 실행\n",
    "                predicted_text = recognizer.predict(test_image_path)\n",
    "                \n",
    "                print(f\"\\n--- 테스트 결과 ---\")\n",
    "                print(f\"이미지: {test_image_path}\")\n",
    "                print(f\"예측된 텍스트: '{predicted_text}'\")\n",
    "            except (IndexError, FileNotFoundError):\n",
    "                print(\"테스트할 이미지를 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(\"문자셋 생성에 실패하여 학습을 시작할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396800b6-ecc2-4de5-95d2-186475d7ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Tensor210Py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\n",
      "==================================================\n",
      "CHARACTER_SET 생성이 완료되었습니다.\n",
      "총 글자 수: 1550\n",
      "==================================================\n",
      "인식 모델의 클래스 개수 (blank 포함): 1551\n",
      "\n",
      "===== 모델 성능 평가 시작 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20656\\1995971611.py:217: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20656\\1995971611.py:134: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(height, width, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizer가 'crnn_recognizer_final.pth' 모델을 성공적으로 로드했습니다.\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00002547.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00007143.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00012858.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00044170.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00097253.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00123738.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00127828.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00208717.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00249756.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00329351.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00345028.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 평가 중: 100%|██████████████████████████████████████████████████████████████| 12175/12175 [30:22<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 샘플 수: 389574\n",
      "정답 수: 383212\n",
      "정확도: 98.37%\n",
      "===== 모델 성능 평가 완료 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader # DataLoader는 평가를 위해 필요\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 100\n",
    "\n",
    "# ====================================================================================\n",
    "# 0. 유틸리티 함수: 문자셋 생성\n",
    "# ====================================================================================\n",
    "def generate_character_set(gt_file):\n",
    "    \"\"\"gt.txt 파일에서 모든 고유 문자를 추출하여 문자셋을 생성합니다.\"\"\"\n",
    "    if not os.path.exists(gt_file):\n",
    "        print(f\"오류: gt.txt 파일을 찾을 수 없습니다! 경로를 확인하세요: {gt_file}\")\n",
    "        return None\n",
    "\n",
    "    all_characters = set()\n",
    "    with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                _, text = line.strip().split('\\t', 1)\n",
    "                for char in text:\n",
    "                    all_characters.add(char)\n",
    "            except ValueError:\n",
    "                # 탭으로 분리되지 않은 줄은 건너뜁니다.\n",
    "                continue\n",
    "\n",
    "    sorted_characters = sorted(list(all_characters))\n",
    "    final_charset = \"\".join(sorted_characters)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"CHARACTER_SET 생성이 완료되었습니다.\")\n",
    "    print(f\"총 글자 수: {len(final_charset)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return final_charset\n",
    "\n",
    "# ====================================================================================\n",
    "# 1. 레이블 변환기 (CTCLabelConverter)\n",
    "# ====================================================================================\n",
    "class CTCLabelConverter:\n",
    "    \"\"\"텍스트와 인덱스 간의 변환을 담당하는 클래스\"\"\"\n",
    "    def __init__(self, character_set):\n",
    "        # 0번 인덱스는 CTC Loss를 위한 'blank' 토큰으로 예약합니다.\n",
    "        self.character_set = [\"-\"] + list(character_set)\n",
    "        \n",
    "        # 문자를 인덱스로 변환하는 딕셔너리\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.character_set)}\n",
    "        # 인덱스를 문자로 변환하는 딕셔너리\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.character_set)}\n",
    "        \n",
    "        print(f\"인식 모델의 클래스 개수 (blank 포함): {self.get_num_classes()}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"입력된 텍스트 문자열을 숫자 인덱스의 리스트로 변환합니다.\"\"\"\n",
    "        indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"모델의 출력(인덱스 시퀀스)을 텍스트 문자열로 디코딩합니다.\"\"\"\n",
    "        text = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx_item = idx.item()\n",
    "            if idx_item == 0:  # blank 토큰은 무시\n",
    "                last_idx = 0\n",
    "                continue\n",
    "            if idx_item == last_idx:  # 연속 중복 문자 무시\n",
    "                continue\n",
    "            \n",
    "            text.append(self.idx_to_char[idx_item])\n",
    "            last_idx = idx_item\n",
    "            \n",
    "        return \"\".join(text)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        \"\"\"blank 토큰을 포함한 전체 클래스의 개수를 반환합니다.\"\"\"\n",
    "        return len(self.character_set)\n",
    "\n",
    "# ====================================================================================\n",
    "# 2. 데이터 파이프라인 (Dataset for Evaluation)\n",
    "# ====================================================================================\n",
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"인식용 데이터셋을 위한 클래스\"\"\"\n",
    "    def __init__(self, gt_file_path, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        with open(gt_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    filename, text = line.strip().split('\\t', 1) # 탭 기준으로 한 번만 분리\n",
    "                    self.samples.append((filename, text))\n",
    "                except ValueError:\n",
    "                    print(f\"경고: 잘못된 형식의 라인 발견 - {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, text = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # 이미지를 읽을 수 없는 경우 빈 이미지와 텍스트를 반환하거나 오류 처리\n",
    "            # 여기서는 편의상 빈 이미지와 텍스트를 반환하고 경고를 출력합니다.\n",
    "            print(f\"경고: 이미지를 읽을 수 없습니다. 건너뛰기: {image_path}\")\n",
    "            # 대체 이미지 생성 (예: 검은색 이미지)\n",
    "            image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8) \n",
    "            text = \"\" # 이 샘플의 텍스트도 비웁니다.\n",
    "            \n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            \n",
    "        return image, text # 이미지 읽기 실패 시 image는 np.array, 성공 시 torch.Tensor\n",
    "\n",
    "\n",
    "def get_recognition_transforms(height, width):\n",
    "    \"\"\"인식 모델 추론을 위한 데이터 전처리 파이프라인을 정의합니다.\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height, width, always_apply=True),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def recognition_collate_fn_for_inference(batch):\n",
    "    \"\"\"\n",
    "    추론 시 가변 길이 텍스트를 처리하기 위한 커스텀 collate 함수.\n",
    "    이 함수는 학습 시와 달리 텍스트를 인코딩할 필요 없이 그대로 반환합니다.\n",
    "    \"\"\"\n",
    "    images, texts = zip(*batch)\n",
    "    \n",
    "    # 필터링: 유효한 이미지(torch.Tensor)만 필터링합니다.\n",
    "    valid_images = [img for img in images if isinstance(img, torch.Tensor)]\n",
    "    valid_texts = [text for i, text in enumerate(texts) if isinstance(images[i], torch.Tensor)]\n",
    "\n",
    "    if not valid_images:\n",
    "        return None, None # 유효한 이미지가 없는 경우\n",
    "\n",
    "    images = torch.stack(valid_images, 0)\n",
    "    return images, valid_texts\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# 3. 텍스트 인식 모델 (CRNN) 아키텍처\n",
    "# ====================================================================================\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars, rnn_hidden_size=256, rnn_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. CNN 특징 추출기 (VGG 스타일) ---\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 64, 16, 50)\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 128, 8, 25)\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 256, 4, 25)\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 512, 2, 25)\n",
    "            nn.Conv2d(512, 512, (2,1), 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)  # -> (B, 512, 1, 25)\n",
    "        )\n",
    "        \n",
    "        # --- 2. RNN (LSTM) 문맥 학습기 ---\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512,             # CNN 출력의 채널 수\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=rnn_layers,\n",
    "            bidirectional=True,         # 양방향 RNN으로 더 넓은 문맥 파악\n",
    "            dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier (분류기) ---\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_chars) # 양방향이므로 *2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. CNN을 통과시켜 이미지 특징 추출\n",
    "        features = self.cnn(x)  # -> (Batch, Channels, Height, Width) = (B, 512, 1, 25)\n",
    "        \n",
    "        # 2. RNN 입력 형식으로 변환: (SeqLen, Batch, InputSize)\n",
    "        b, c, h, w = features.size()\n",
    "        assert h == 1, \"CNN 출력의 높이는 1이어야 합니다.\"\n",
    "        features = features.squeeze(2)      # 높이(H) 차원 제거 -> (B, 512, 25)\n",
    "        features = features.permute(2, 0, 1)  # 차원 순서 변경 -> (W, B, C) = (25, B, 512)\n",
    "        \n",
    "        # 3. RNN을 통과시켜 문맥 정보 학습\n",
    "        rnn_output, _ = self.rnn(features) # -> (SeqLen, Batch, HiddenSize*2)\n",
    "        \n",
    "        # 4. 각 시퀀스 스텝에 대해 문자 분류\n",
    "        output = self.classifier(rnn_output) # -> (SeqLen, Batch, NumClasses)\n",
    "        return output\n",
    "\n",
    "# ====================================================================================\n",
    "# 4. 학습된 모델을 사용한 추론(Inference) 및 평가 모듈\n",
    "# ====================================================================================\n",
    "class Recognizer:\n",
    "    \"\"\"학습된 CRNN 모델을 사용하여 텍스트 인식을 수행하는 클래스\"\"\"\n",
    "    def __init__(self, model_path, converter, device=\"cpu\", img_height=32, img_width=100):\n",
    "        self.device = device\n",
    "        self.converter = converter\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "        # 모델 로드 (학습 시 사용한 CRNN 아키텍처와 동일하게 초기화)\n",
    "        self.model = CRNN(num_chars=self.converter.get_num_classes()).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval() # 추론 모드로 설정\n",
    "\n",
    "        # 추론 시 사용할 이미지 전처리 (학습 시와 동일한 정규화)\n",
    "        self.transform = get_recognition_transforms(self.img_height, self.img_width)\n",
    "        print(f\"Recognizer가 '{model_path}' 모델을 성공적으로 로드했습니다.\")\n",
    "\n",
    "    def predict(self, image_input):\n",
    "        \"\"\"\n",
    "        이미지 파일 경로 또는 이미 전처리된 PyTorch 텐서로부터 텍스트를 예측합니다.\n",
    "        Args:\n",
    "            image_input (str or torch.Tensor): 예측할 이미지 파일의 경로 또는 (1, C, H, W) 형태의 PyTorch 텐서.\n",
    "        Returns:\n",
    "            str: 예측된 텍스트.\n",
    "        \"\"\"\n",
    "        if isinstance(image_input, str): # 이미지 경로가 주어졌을 경우\n",
    "            image_path = image_input\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"오류: 이미지를 찾을 수 없습니다: {image_path}\")\n",
    "                return \"\"\n",
    "\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"오류: 이미지를 읽을 수 없습니다. 경로 또는 파일 손상 확인: {image_path}\")\n",
    "                return \"\"\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            processed_image = self.transform(image=image)['image']\n",
    "            processed_image = processed_image.unsqueeze(0).to(self.device) # 배치 차원 추가\n",
    "        elif isinstance(image_input, torch.Tensor): # 이미 텐서 형태일 경우\n",
    "            processed_image = image_input.to(self.device)\n",
    "            if processed_image.dim() == 3: # (C, H, W) 형태면 배치 차원 추가\n",
    "                processed_image = processed_image.unsqueeze(0)\n",
    "        else:\n",
    "            print(\"오류: image_input은 이미지 경로(str) 또는 PyTorch 텐서여야 합니다.\")\n",
    "            return \"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model(processed_image)\n",
    "            preds_indices = output.argmax(2).permute(1, 0) # (SeqLen, Batch, NumClasses) -> (SeqLen, Batch) -> (Batch, SeqLen)\n",
    "            \n",
    "            decoded_text = self.converter.decode(preds_indices[0]) # 첫 번째 (유일한) 배치 샘플\n",
    "        return decoded_text\n",
    "\n",
    "def evaluate_recognizer_performance(model_path, converter, gt_file_path, image_dir, device=\"cpu\", batch_size=32):\n",
    "    \"\"\"\n",
    "    학습된 모델의 성능을 평가하는 함수.\n",
    "    Args:\n",
    "        model_path (str): 학습된 모델(.pth) 파일 경로.\n",
    "        converter (CTCLabelConverter): 레이블 변환기 객체.\n",
    "        gt_file_path (str): Ground Truth 텍스트 파일 경로.\n",
    "        image_dir (str): 이미지 파일들이 저장된 디렉토리 경로.\n",
    "        device (str): 모델을 로드할 디바이스 ('cuda' 또는 'cpu').\n",
    "        batch_size (int): 평가 시 사용할 배치 크기.\n",
    "    \"\"\"\n",
    "    print(\"\\n===== 모델 성능 평가 시작 =====\")\n",
    "    recognizer = Recognizer(model_path, converter, device, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "    # 평가 데이터셋 및 DataLoader 준비\n",
    "    eval_dataset = RecognitionDataset(\n",
    "        gt_file_path=gt_file_path,\n",
    "        image_dir=image_dir,\n",
    "        transform=get_recognition_transforms(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "\n",
    "    if len(eval_dataset) == 0:\n",
    "        print(\"오류: 평가할 데이터셋에 샘플이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, # 평가 시에는 섞을 필요 없음\n",
    "        num_workers=0, # Windows 환경에서는 0이 안정적\n",
    "        collate_fn=recognition_collate_fn_for_inference\n",
    "    )\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for images, ground_truth_texts in tqdm(eval_loader, desc=\"모델 평가 중\"):\n",
    "        if images is None or ground_truth_texts is None: # 이미지 로드 실패 등으로 인해 유효한 배치가 아닌 경우\n",
    "            continue\n",
    "            \n",
    "        # 모델 예측\n",
    "        with torch.no_grad():\n",
    "            outputs = recognizer.model(images.to(device))\n",
    "            preds_indices = outputs.argmax(2).permute(1, 0) # (Batch, SeqLen)\n",
    "\n",
    "        # 예측 결과 디코딩 및 비교\n",
    "        for i in range(preds_indices.size(0)):\n",
    "            predicted_text = recognizer.converter.decode(preds_indices[i])\n",
    "            ground_truth_text = ground_truth_texts[i] # 해당 배치 샘플의 실제 텍스트\n",
    "\n",
    "            total_samples += 1\n",
    "            if predicted_text == ground_truth_text:\n",
    "                correct_predictions += 1\n",
    "            # else: # 틀린 예측을 보고 싶다면 주석 해제\n",
    "            #     print(f\"GT: '{ground_truth_text:<20}' | PRED: '{predicted_text}'\")\n",
    "\n",
    "    accuracy = (correct_predictions / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"\\n총 샘플 수: {total_samples}\")\n",
    "    print(f\"정답 수: {correct_predictions}\")\n",
    "    print(f\"정확도: {accuracy:.2f}%\")\n",
    "    print(\"===== 모델 성능 평가 완료 =====\")\n",
    "\n",
    "\n",
    "# ====================================================================================\n",
    "# 메인 실행 블록\n",
    "# ====================================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- [1단계: 설정] 사용하려는 모델 파일 및 데이터 경로를 여기에 정의합니다. ---\n",
    "    # !!! 중요: 여기에 실제 모델 파일 경로와 gt.txt, 이미지 폴더 경로를 넣어주세요 !!!\n",
    "    # 예시:\n",
    "    MODEL_PATH = \"crnn_recognizer_final.pth\" # 저장된 모델 파일 이름\n",
    "    GT_FILE_PATH = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\gt.txt\" # gt.txt 파일 경로\n",
    "    IMAGE_DIR = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\images\"    # 이미지 폴더 경로\n",
    "\n",
    "    # --- [2단계: 문자셋 생성] gt.txt 파일로부터 문자셋을 생성합니다. ---\n",
    "    print(\">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\")\n",
    "    charset = generate_character_set(GT_FILE_PATH)\n",
    "    \n",
    "    if charset:\n",
    "        # --- [3단계: CTCLabelConverter 초기화] ---\n",
    "        label_converter = CTCLabelConverter(charset)\n",
    "\n",
    "        # --- [4단계: 모델 로드 및 성능 평가] ---\n",
    "        # test_recognizer_performance 함수를 호출하여 모델을 로드하고 평가를 시작합니다.\n",
    "        evaluate_recognizer_performance(MODEL_PATH, label_converter, GT_FILE_PATH, IMAGE_DIR, DEVICE)\n",
    "    else:\n",
    "        print(\"문자셋 생성에 실패하여 모델 평가를 시작할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5f34e-7cb4-418a-846c-43f87e95ccec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed1430d-5c3e-4bce-8b94-7d57b2e3f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\n",
      "==================================================\n",
      "CHARACTER_SET 생성이 완료되었습니다.\n",
      "총 글자 수: 1550\n",
      "==================================================\n",
      "인식 모델의 클래스 개수 (blank 포함): 1551\n",
      "\n",
      ">> 2. 모델 'crnn_recognizer_final.pth'를 로드합니다...\n",
      "모델 로드 성공.\n",
      "\n",
      ">> 3. 평가 데이터셋을 준비합니다...\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00002547.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00007143.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00012858.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00044170.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00097253.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20656\\968197438.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_20656\\968197438.py:135: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(height, width, always_apply=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00123738.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00127828.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00208717.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00249756.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00329351.png\n",
      "경고: 잘못된 형식의 라인 발견 - rec_crop_00345028.png\n",
      "총 389574개의 샘플을 평가합니다.\n",
      "\n",
      ">> 4. 모델 성능 평가를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "모델 평가 진행 중: 100%|█████████████████████████████████████████████████████████| 12175/12175 [15:58<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 모델 성능 평가 결과 =====\n",
      "총 평가 샘플 수: 389574\n",
      "문자열 완전 일치 정답 수: 383212\n",
      "정확도 (문자열 완전 일치 기준): 98.37%\n",
      "문자 오류율 (Character Error Rate, CER): 1.77%\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import jiwer # CER 계산을 위해 필요합니다.\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 100\n",
    "\n",
    "# ====================================================================================\n",
    "# 0. 유틸리티 함수: 문자셋 생성\n",
    "# ====================================================================================\n",
    "def generate_character_set(gt_file):\n",
    "    \"\"\"gt.txt 파일에서 모든 고유 문자를 추출하여 문자셋을 생성합니다.\"\"\"\n",
    "    if not os.path.exists(gt_file):\n",
    "        print(f\"오류: gt.txt 파일을 찾을 수 없습니다! 경로를 확인하세요: {gt_file}\")\n",
    "        return None\n",
    "\n",
    "    all_characters = set()\n",
    "    with open(gt_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                # 파일명과 텍스트가 탭으로 구분되어 있다고 가정합니다.\n",
    "                _, text = line.strip().split('\\t', 1) \n",
    "                for char in text:\n",
    "                    all_characters.add(char)\n",
    "            except ValueError:\n",
    "                # 탭으로 분리되지 않은 줄은 건너뜁니다.\n",
    "                continue\n",
    "\n",
    "    sorted_characters = sorted(list(all_characters))\n",
    "    final_charset = \"\".join(sorted_characters)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"CHARACTER_SET 생성이 완료되었습니다.\")\n",
    "    print(f\"총 글자 수: {len(final_charset)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return final_charset\n",
    "\n",
    "# ====================================================================================\n",
    "# 1. 레이블 변환기 (CTCLabelConverter)\n",
    "# ====================================================================================\n",
    "class CTCLabelConverter:\n",
    "    \"\"\"텍스트와 인덱스 간의 변환을 담당하는 클래스\"\"\"\n",
    "    def __init__(self, character_set):\n",
    "        # 0번 인덱스는 CTC Loss를 위한 'blank' 토큰으로 예약합니다.\n",
    "        self.character_set = [\"-\"] + list(character_set)\n",
    "        \n",
    "        # 문자를 인덱스로 변환하는 딕셔너리\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.character_set)}\n",
    "        # 인덱스를 문자로 변환하는 딕셔너리\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.character_set)}\n",
    "        \n",
    "        print(f\"인식 모델의 클래스 개수 (blank 포함): {self.get_num_classes()}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"입력된 텍스트 문자열을 숫자 인덱스의 리스트로 변환합니다.\"\"\"\n",
    "        indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"모델의 출력(인덱스 시퀀스)을 텍스트 문자열로 디코딩합니다.\"\"\"\n",
    "        text = []\n",
    "        last_idx = 0\n",
    "        for idx in indices:\n",
    "            idx_item = idx.item()\n",
    "            if idx_item == 0:  # blank 토큰은 무시 (CTC Blank)\n",
    "                last_idx = 0\n",
    "                continue\n",
    "            if idx_item == last_idx:  # 연속 중복 문자 무시 (CTC Collapse)\n",
    "                continue\n",
    "            \n",
    "            text.append(self.idx_to_char[idx_item])\n",
    "            last_idx = idx_item\n",
    "            \n",
    "        return \"\".join(text)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        \"\"\"blank 토큰을 포함한 전체 클래스의 개수를 반환합니다.\"\"\"\n",
    "        return len(self.character_set)\n",
    "\n",
    "# ====================================================================================\n",
    "# 2. 데이터 파이프라인 (Dataset for Evaluation)\n",
    "# ====================================================================================\n",
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"인식용 데이터셋을 위한 클래스 (평가용)\"\"\"\n",
    "    def __init__(self, gt_file_path, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        with open(gt_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    filename, text = line.strip().split('\\t', 1) \n",
    "                    self.samples.append((filename, text))\n",
    "                except ValueError:\n",
    "                    print(f\"경고: 잘못된 형식의 라인 발견 - {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, text = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            # 이미지를 읽을 수 없는 경우, 경고 출력 및 대체 이미지/텍스트 반환\n",
    "            print(f\"경고: 이미지를 읽을 수 없습니다. 건너뛰기: {image_path}\")\n",
    "            # 이 경우 해당 샘플은 평가에서 제외될 수 있도록 None을 반환하거나,\n",
    "            # 특정 값을 반환하여 collate_fn에서 처리하게 할 수 있습니다.\n",
    "            # 여기서는 편의상 numpy 배열로 된 0 값을 반환하여 이후 필터링합니다.\n",
    "            return np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8), \"\" \n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "            \n",
    "        return image, text \n",
    "\n",
    "\n",
    "def get_recognition_transforms(height, width):\n",
    "    \"\"\"인식 모델 추론을 위한 데이터 전처리 파이프라인을 정의합니다.\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(height, width, always_apply=True),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def recognition_collate_fn_for_inference(batch):\n",
    "    \"\"\"\n",
    "    추론 시 가변 길이 텍스트를 처리하기 위한 커스텀 collate 함수.\n",
    "    이미지 로드 실패 샘플을 필터링하고, 유효한 배치만 구성합니다.\n",
    "    \"\"\"\n",
    "    images, texts = zip(*batch)\n",
    "    \n",
    "    # 이미지 로드 실패 등으로 인해 np.array (0 값)가 반환된 경우를 필터링\n",
    "    # 유효한 PyTorch 텐서만 모읍니다.\n",
    "    valid_samples = [(img, text) for img, text in zip(images, texts) if isinstance(img, torch.Tensor)]\n",
    "\n",
    "    if not valid_samples:\n",
    "        return None, None # 유효한 이미지가 없는 경우\n",
    "\n",
    "    valid_images, valid_texts = zip(*valid_samples)\n",
    "    images_tensor = torch.stack(valid_images, 0)\n",
    "    return images_tensor, list(valid_texts) # texts는 리스트 형태로 유지\n",
    "\n",
    "# ====================================================================================\n",
    "# 3. 텍스트 인식 모델 (CRNN) 아키텍처 (학습 시와 동일해야 합니다)\n",
    "# ====================================================================================\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars, rnn_hidden_size=256, rnn_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- 1. CNN 특징 추출기 (VGG 스타일) ---\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 64, 16, 50)\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # -> (B, 128, 8, 25)\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 256, 4, 25)\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2, 1), (2, 1)), # -> (B, 512, 2, 25)\n",
    "            nn.Conv2d(512, 512, (2,1), 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)  # -> (B, 512, 1, 25)\n",
    "        )\n",
    "        \n",
    "        # --- 2. RNN (LSTM) 문맥 학습기 ---\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512,             # CNN 출력의 채널 수\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            num_layers=rnn_layers,\n",
    "            bidirectional=True,         # 양방향 RNN으로 더 넓은 문맥 파악\n",
    "            dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # --- 3. Classifier (분류기) ---\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_chars) # 양방향이므로 *2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. CNN을 통과시켜 이미지 특징 추출\n",
    "        features = self.cnn(x)  # -> (Batch, Channels, Height, Width) = (B, 512, 1, 25)\n",
    "        \n",
    "        # 2. RNN 입력 형식으로 변환: (SeqLen, Batch, InputSize)\n",
    "        b, c, h, w = features.size()\n",
    "        assert h == 1, \"CNN 출력의 높이는 1이어야 합니다.\"\n",
    "        features = features.squeeze(2)      # 높이(H) 차원 제거 -> (B, 512, 25)\n",
    "        features = features.permute(2, 0, 1)  # 차원 순서 변경 -> (W, B, C) = (25, B, 512)\n",
    "        \n",
    "        # 3. RNN을 통과시켜 문맥 정보 학습\n",
    "        rnn_output, _ = self.rnn(features) # -> (SeqLen, Batch, HiddenSize*2)\n",
    "        \n",
    "        # 4. 각 시퀀스 스텝에 대해 문자 분류\n",
    "        output = self.classifier(rnn_output) # -> (SeqLen, Batch, NumClasses)\n",
    "        return output\n",
    "\n",
    "# ====================================================================================\n",
    "# 4. 학습된 모델 로드 및 성능 평가 스크립트\n",
    "# ====================================================================================\n",
    "def main():\n",
    "    # --- [1단계: 설정] 사용하려는 모델 파일 및 데이터 경로를 여기에 정의합니다. ---\n",
    "    # !!! 중요: 여기에 실제 모델 파일 경로와 gt.txt, 이미지 폴더 경로를 넣어주세요 !!!\n",
    "    MODEL_PATH = \"crnn_recognizer_final.pth\" \n",
    "    GT_FILE_PATH = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\gt.txt\" \n",
    "    IMAGE_DIR = r\"C:\\Users\\User\\DBNet_OCR\\data\\crop\\images\"    \n",
    "\n",
    "    # --- [2단계: 문자셋 생성] gt.txt 파일로부터 문자셋을 생성합니다. ---\n",
    "    print(\">> 1. 문자셋(CHARACTER_SET)을 생성합니다...\")\n",
    "    charset = generate_character_set(GT_FILE_PATH)\n",
    "    \n",
    "    if charset is None: # 문자셋 생성 실패 시 종료\n",
    "        print(\"문자셋 생성에 실패하여 모델 평가를 시작할 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # --- [3단계: CTCLabelConverter 초기화] ---\n",
    "    label_converter = CTCLabelConverter(charset)\n",
    "\n",
    "    # --- [4단계: 모델 로드] ---\n",
    "    print(f\"\\n>> 2. 모델 '{MODEL_PATH}'를 로드합니다...\")\n",
    "    try:\n",
    "        model = CRNN(num_chars=label_converter.get_num_classes()).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "        model.eval() # 평가 모드 설정\n",
    "        print(\"모델 로드 성공.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 모델 파일 '{MODEL_PATH}'을(를) 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 모델 로드 중 문제가 발생했습니다: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- [5단계: 데이터셋 및 DataLoader 준비 (평가용)] ---\n",
    "    print(\"\\n>> 3. 평가 데이터셋을 준비합니다...\")\n",
    "    eval_dataset = RecognitionDataset(\n",
    "        gt_file_path=GT_FILE_PATH,\n",
    "        image_dir=IMAGE_DIR,\n",
    "        transform=get_recognition_transforms(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "\n",
    "    if len(eval_dataset) == 0:\n",
    "        print(\"오류: 평가할 데이터셋에 샘플이 없습니다. gt.txt와 이미지 폴더를 확인해주세요.\")\n",
    "        return\n",
    "\n",
    "    # 배치 사이즈는 평가 시 자유롭게 설정할 수 있습니다.\n",
    "    EVAL_BATCH_SIZE = 32 \n",
    "    eval_loader = DataLoader(\n",
    "        dataset=eval_dataset,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        shuffle=False, # 평가 시에는 데이터 순서를 섞을 필요가 없습니다.\n",
    "        num_workers=0, # Windows 환경에서는 0이 안정적입니다.\n",
    "        collate_fn=recognition_collate_fn_for_inference # 커스텀 collate 함수 사용\n",
    "    )\n",
    "    print(f\"총 {len(eval_dataset)}개의 샘플을 평가합니다.\")\n",
    "\n",
    "    # --- [6단계: 모델 성능 평가] ---\n",
    "    print(\"\\n>> 4. 모델 성능 평가를 시작합니다...\")\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0 # 문자열 완전 일치 기준\n",
    "    \n",
    "    all_ground_truths = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for images, ground_truth_texts in tqdm(eval_loader, desc=\"모델 평가 진행 중\"):\n",
    "        if images is None or ground_truth_texts is None:\n",
    "            # collate_fn에서 필터링된 유효하지 않은 배치 건너뛰기\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad(): # 그래디언트 계산 비활성화 (메모리 절약 및 속도 향상)\n",
    "            outputs = model(images.to(DEVICE)) # 이미지 텐서를 모델 입력으로 사용\n",
    "            # CTC 디코딩을 위해 예측된 확률 분포에서 가장 높은 확률의 인덱스 선택\n",
    "            # outputs: (SeqLen, Batch, NumClasses) -> argmax(2) -> (SeqLen, Batch)\n",
    "            # -> permute(1, 0) -> (Batch, SeqLen)\n",
    "            preds_indices = outputs.argmax(2).permute(1, 0) \n",
    "\n",
    "        # 배치 내 각 샘플에 대해 예측 결과 디코딩 및 비교\n",
    "        for i in range(preds_indices.size(0)):\n",
    "            predicted_text = label_converter.decode(preds_indices[i])\n",
    "            ground_truth_text = ground_truth_texts[i] # 해당 배치 샘플의 실제 텍스트\n",
    "\n",
    "            total_samples += 1\n",
    "            if predicted_text == ground_truth_text:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            # CER/WER 계산을 위해 예측과 실제 텍스트 저장\n",
    "            all_ground_truths.append(ground_truth_text)\n",
    "            all_predictions.append(predicted_text)\n",
    "\n",
    "    # --- [7단계: 결과 출력] ---\n",
    "    accuracy = (correct_predictions / total_samples) * 100 if total_samples > 0 else 0\n",
    "    \n",
    "    print(\"\\n===== 모델 성능 평가 결과 =====\")\n",
    "    print(f\"총 평가 샘플 수: {total_samples}\")\n",
    "    print(f\"문자열 완전 일치 정답 수: {correct_predictions}\")\n",
    "    print(f\"정확도 (문자열 완전 일치 기준): {accuracy:.2f}%\")\n",
    "\n",
    "    # CER (Character Error Rate) 계산\n",
    "    # jiwer 라이브러리를 사용하여 CER을 계산합니다.\n",
    "    try:\n",
    "        # jiwer.measures.cer 함수는 리스트의 리스트를 기대할 수 있으므로, 단일 문자열 리스트로 전달\n",
    "        # jiwer는 내부적으로 단어를 토큰화하므로, OCR에서는 주로 문자 단위의 비교가 중요합니다.\n",
    "        # 따라서, 문자 단위로 CER을 계산하려면 각 문자열을 공백으로 구분된 문자열로 변환하는 것이 일반적입니다.\n",
    "        # 예: \"hello\" -> \"h e l l o\"\n",
    "        \n",
    "        # 문자를 띄어쓰기로 분리하여 CER 계산 (더 정확한 문자 단위 CER)\n",
    "        processed_ground_truths = [\" \".join(list(s)) for s in all_ground_truths]\n",
    "        processed_predictions = [\" \".join(list(s)) for s in all_predictions]\n",
    "\n",
    "        cer_value = jiwer.cer(processed_ground_truths, processed_predictions)\n",
    "        print(f\"문자 오류율 (Character Error Rate, CER): {cer_value * 100:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"CER 계산 중 오류 발생: {e}. 'pip install jiwer'를 실행했는지 확인해주세요.\")\n",
    "    \n",
    "    print(\"==============================\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.10 (Py3.10)",
   "language": "python",
   "name": "tensortf210py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
